{
  "code_analysis": {
    "focus_areas": {
      "ai_components": {
        "patterns": ["gaia/agents/**/*.py", "gaia/llm/**/*.py"],
        "key_aspects": [
          "Model integration",
          "RAG implementation",
          "Agent behavior",
          "Tool usage",
          "LLM server management",
          "Hybrid mode support"
        ]
      },
      "ui_components": {
        "patterns": ["gaia/interface/**/*"],
        "key_aspects": [
          "User interaction flow",
          "Error handling",
          "State management",
          "UI/UX consistency",
          "Hybrid mode UI"
        ]
      },
      "testing": {
        "patterns": ["tests/**/*.py"],
        "key_aspects": [
          "Unit tests",
          "Integration tests",
          "Hybrid mode testing",
          "Server management testing",
          "LLM functionality testing"
        ]
      }
    },
    "critical_paths": {
      "model_execution": {
        "description": "LLM execution and integration with OGA/Lemonade/Ollama",
        "key_files": [
          "gaia/cli.py",
          "gaia/agents/agent.py",
          "gaia/llm/llama_index_local.py"
        ]
      },
      "server_management": {
        "description": "LLM and Ollama server management",
        "key_files": [
          "gaia/cli.py",
          "tests/test_llama_index.py",
          "tests/test_gaia.py"
        ]
      }
    },
    "dependencies": {
      "external": [
        "ONNX Runtime GenAI",
        "Lemonade",
        "llama_index",
        "Ollama",
        "PySide6",
        "TurnkeyML"
      ],
      "internal": {
        "raux": "Optional UI component, separate installation"
      },
      "models": {
        "hybrid": "amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid",
        "ollama": "llama3.2:1b"
      }
    }
  }
} 