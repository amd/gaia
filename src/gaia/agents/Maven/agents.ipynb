{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Maven Super Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "# Setting environment variables\n",
    "os.environ['OPENAI_API_KEY'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"AVX512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\"pdf\",\n",
    "           \"wikipedia\",\n",
    "           \"arxiv\",\n",
    "           \"youtube\",\n",
    "           \"webpage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents_pdf = SimpleDirectoryReader(\"data/local_pdfs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The SIMD width of AVX-512 is 512 bits."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PDF unit testing\n",
    "index_pdf = VectorStoreIndex.from_documents(documents_pdf)\n",
    "index_pdf.storage_context.persist(persist_dir=\"pdf_reader\")\n",
    "query_engine_pdf = index_pdf.as_query_engine()\n",
    "response = query_engine_pdf.query(\"What is SIMD width of avx 512?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia pages\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "documents_wiki = WikipediaReader().load_data(\n",
    "    pages=[research_topic]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Wiki reader\n",
    "\n",
    "import wikipedia\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "\n",
    "def clean_wikipedia_text(page_text):\n",
    "    lines = page_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    in_reference_section = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip lines that are part of the references section\n",
    "        if \"References\" in line:\n",
    "            in_reference_section = True\n",
    "        if in_reference_section:\n",
    "            continue\n",
    "        \n",
    "        # Skip lines that are part of the contents or media\n",
    "        if line.strip().lower().startswith(('contents', 'references', 'external links', 'see also', 'notes', 'further reading')):\n",
    "            continue\n",
    "        \n",
    "        # Skip lines that are empty\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def fetch_clean_wikipedia_page(topic):\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        cleaned_text = clean_wikipedia_text(page.content)\n",
    "        return cleaned_text, page.title\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Wikipedia page for topic '{topic}' does not exist.\")\n",
    "        return None, None\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation error for topic '{topic}'. Options are: {e.options}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "research_topic = \"AVX-512\"\n",
    "cleaned_page_text, page_title = fetch_clean_wikipedia_page(research_topic)\n",
    "\n",
    "if cleaned_page_text:\n",
    "    # Define the directory and file name\n",
    "    output_dir = \"./wiki_docs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    output_file_path = os.path.join(output_dir, f\"{research_topic}.txt\")\n",
    "    \n",
    "    # Write the cleaned text to a file\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(cleaned_page_text)\n",
    "    \n",
    "    # Load the file using SimpleDirectoryReader\n",
    "    reader = SimpleDirectoryReader(input_dir=output_dir)\n",
    "    documents_wiki = reader.load_data()\n",
    "    \n",
    "else:\n",
    "    print(f\"Could not retrieve or clean Wikipedia page for topic '{research_topic}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Linus Torvalds expressed his strong opinion on AVX-512, stating that he hopes it \"dies a painful death\" and criticized Intel for focusing on creating benchmark-boosting instructions rather than addressing real issues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wikipedia unit testing\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index_wikipedia = VectorStoreIndex.from_documents(documents_wiki)\n",
    "index_wikipedia.storage_context.persist(persist_dir=\"wiki_reader\")\n",
    "query_engine_wiki = index_wikipedia.as_query_engine()\n",
    "response = query_engine_wiki.query(\"What is a spicy quote on avx-512?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AVX-512 are 512-bit extensions to the 256-bit Advanced Vector Extensions SIMD instructions for x86 instruction set architecture (ISA) proposed by Intel in July 2013, and first implemented in the 2016 Intel Xeon Phi x200 (Knights Landing), and then later in a number of AMD and other Intel CPUs (see list below). AVX-512 consists of multiple extensions that may be implemented independently. This policy is a departure from the historical requirement of implementing the entire instruction block. Only the core extension AVX-512F (AVX-512 Foundation) is required by all AVX-512 implementations.\\nBesides widening most 256-bit instructions, the extensions introduce various new operations, such as new data conversions, scatter operations, and permutations. The number of AVX registers is increased from 16 to 32, and eight new \"mask registers\" are added, which allow for variable selection and blending of the results of instructions. In CPUs with the vector length (VL) extension—included in most AVX-512-capable processors (see § CPUs with AVX-512)—these instructions may also be used on the 128-bit and 256-bit vector sizes. AVX-512 is not the first 512-bit SIMD instruction set that Intel has introduced in processors: the earlier 512-bit SIMD instructions used in the first generation Xeon Phi coprocessors, derived from Intel\\'s Larrabee project, are similar but not binary compatible and only partially source compatible.\\n\\n\\n== Instruction set ==\\nThe AVX-512 instruction set consists of several separate sets each having their own unique CPUID feature bit; however, they are typically grouped by the processor generation that implements them.\\n\\nF, CD, ER, PF\\nIntroduced with Xeon Phi x200 (Knights Landing) and Xeon Gold/Platinum (Skylake SP \"Purley\"), with the last two (ER and PF) being specific to Knights Landing.\\nAVX-512 Foundation (F) –  expands most 32-bit and 64-bit based AVX instructions with the EVEX coding scheme to support 512-bit registers, operation masks, parameter broadcasting, and embedded rounding and exception control, implemented by Knights Landing and Skylake Xeon\\nAVX-512 Conflict Detection Instructions (CD) –  efficient conflict detection to allow more loops to be vectorized, implemented by Knights Landing and Skylake X\\nAVX-512 Exponential and Reciprocal Instructions (ER) –  exponential and reciprocal operations designed to help implement transcendental operations, implemented by Knights Landing\\nAVX-512 Prefetch Instructions (PF) –  new prefetch capabilities, implemented by Knights Landing\\nVL, DQ, BW\\nIntroduced with Skylake X and Cannon Lake.\\nAVX-512 Vector Length Extensions (VL) –  extends most AVX-512 operations to also operate on XMM (128-bit) and YMM (256-bit) registers\\nAVX-512 Doubleword and Quadword Instructions (DQ) –  adds new 32-bit and 64-bit AVX-512 instructions\\nAVX-512 Byte and Word Instructions (BW) –  extends AVX-512 to cover 8-bit and 16-bit integer operations\\nIFMA, VBMI\\nIntroduced with Cannon Lake.\\nAVX-512 Integer Fused Multiply Add (IFMA) – fused multiply add of integers using 52-bit precision.\\nAVX-512 Vector Byte Manipulation Instructions (VBMI) adds vector byte permutation instructions which were not present in AVX-512BW.\\n4VNNIW, 4FMAPS\\nIntroduced with Knights Mill.\\nAVX-512 Vector Neural Network Instructions Word variable precision (4VNNIW) – vector instructions for deep learning, enhanced word, variable precision.\\nAVX-512 Fused Multiply Accumulation Packed Single precision (4FMAPS) – vector instructions for deep learning, floating point, single precision.\\nVPOPCNTDQ\\nVector population count instruction. Introduced with Knights Mill and Ice Lake.\\nVNNI, VBMI2, BITALG\\nIntroduced with Ice Lake.\\nAVX-512 Vector Neural Network Instructions (VNNI) – vector instructions for deep learning.\\nAVX-512 Vector Byte Manipulation Instructions 2 (VBMI2) – byte/word load, store and concatenation with shift.\\nAVX-512 Bit Algorithms (BITALG) – byte/word bit manipulation instructions expanding VPOPCNTDQ.\\nVP2INTERSECT\\nIntroduced with Tiger Lake.\\nAVX-512 Vector Pair Intersection to a Pair of Mask Registers (VP2INTERSECT).\\nGFNI, VPCLMULQDQ, VAES\\nIntroduced with Ice Lake.\\nThese are not AVX-512 features per se. Together with AVX-512, they enable EVEX encoded versions of GFNI, PCLMULQDQ and AES instructions.\\n\\n\\n== Encoding and features ==\\nThe VEX prefix used by AVX and AVX2, while flexible, did not leave enough room for the features Intel wanted to add to AVX-512. This has led them to define a new prefix called EVEX.\\nCompared to VEX, EVEX adds the following benefits:\\n\\nExpanded register encoding allowing 32 512-bit registers.\\nAdds 8 new opmask registers for masking most AVX-512 instructions.\\nAdds a new scalar memory mode that automatically performs a broadcast.\\nAdds room for explicit rounding control in each instruction.\\nAdds a new compressed displacement memory addressing mode.\\nThe extended registers, SIMD width bit, and opmask registers of AVX-512 are mandatory and all require support from the OS.\\n\\n\\n=== SIMD modes ===\\nThe AVX-512 instructions are designed to mix with 128/256-bit AVX/AVX2 instructions without a performance penalty. However, AVX-512VL extensions allows the use of AVX-512 instructions on 128/256-bit registers XMM/YMM, so most SSE and AVX/AVX2 instructions have new AVX-512 versions encoded with the EVEX prefix which allow access to new features such as opmask and additional registers. Unlike AVX-256, the new instructions do not have new mnemonics but share namespace with AVX, making the distinction between VEX and EVEX encoded versions of an instruction ambiguous in the source code. Since AVX-512F only works on 32- and 64-bit values, SSE and AVX/AVX2 instructions that operate on bytes or words are available only with the AVX-512BW extension (byte & word support).\\n\\n\\n=== Extended registers ===\\n\\nThe width of the SIMD register file is increased from 256 bits to 512 bits, and expanded from 16 to a total of 32 registers ZMM0–ZMM31. These registers can be addressed as 256 bit YMM registers from AVX extensions and 128-bit XMM registers from Streaming SIMD Extensions, and legacy AVX and SSE instructions can be extended to operate on the 16 additional registers XMM16-XMM31 and YMM16-YMM31 when using EVEX encoded form.\\n\\n\\n== Opmask registers ==\\nAVX-512 vector instructions may indicate an opmask register to control which values are written to the destination, the instruction encoding supports 0–7 for this field, however, only opmask registers k1–k7 (of k0–k7) can be used as the mask corresponding to the value 1–7, whereas the value 0 is reserved for indicating no opmask register is used, i.e. a hardcoded constant (instead of \\'k0\\') is used to indicate unmasked operations. The special opmask register \\'k0\\' is still a functioning, valid register, it can be used in opmask register manipulation instructions or used as the destination opmask register. A flag controls the opmask behavior, which can either be \"zero\", which zeros everything not selected by the mask, or \"merge\", which leaves everything not selected untouched. The merge behavior is identical to the blend instructions.\\nThe opmask registers are normally 16 bits wide, but can be up to 64 bits with the AVX-512BW extension. How many of the bits are actually used, though, depends on the vector type of the instructions masked. For the 32-bit single float or double words, 16 bits are used to mask the 16 elements in a 512-bit register. For double float and quad words, at most 8 mask bits are used.\\nThe opmask register is the reason why several bitwise instructions which naturally have no element widths had them added in AVX-512. For instance, bitwise AND, OR or 128-bit shuffle now exist in both double-word and quad-word variants with the only difference being in the final masking.\\n\\n\\n=== New opmask instructions ===\\nThe opmask registers have a new mini extension of instructions operating directly on them. Unlike the rest of the AVX-512 instructions, these instructions are all VEX encoded. The initial opmask instructions are all 16-bit (Word) versions. With AVX-512DQ 8-bit (Byte) versions were added to better match the needs of masking 8 64-bit values, and with AVX-512BW 32-bit (Double) and 64-bit (Quad) versions were added so they can mask up to 64 8-bit values. The instructions KORTEST and KTEST can be used to set the x86 flags based on mask registers, so that they may be used together with non-SIMD x86 branch and conditional instructions.\\n\\n\\n== New instructions in AVX-512 foundation ==\\nMany AVX-512 instructions are simply EVEX versions of old SSE or AVX instructions. There are, however, several new instructions, and old instructions that have been replaced with new AVX-512 versions. The new or heavily reworked instructions are listed below. These foundation instructions also include the extensions from AVX-512VL and AVX-512BW since those extensions merely add new versions of these instructions instead of new instructions.\\n\\n\\n=== Blend using mask ===\\nThere are no EVEX-prefixed versions of the blend instructions from SSE4; instead, AVX-512 has a new set of blending instructions using mask registers as selectors. Together with the general compare into mask instructions below, these may be used to implement generic ternary operations or cmov, similar to XOP\\'s VPCMOV.\\nSince blending is an integral part of the EVEX encoding, these instructions may also be considered basic move instructions. Using the zeroing blend mode, they can also be used as masking instructions.\\n\\n\\n=== Compare into mask ===\\nAVX-512F has four new compare instructions. Like their XOP counterparts they use the immediate field to select between 8 different comparisons. Unlike their XOP inspiration, however, they save the result to a mask register and initially only support doubleword and quadword comparisons. The AVX-512BW extension provides the byte and word versions. Note that two mask registers may be specified for the instructions, one to write to and one to declare regular masking.\\n\\n\\n=== Logical set mask ===\\nThe final way to set masks is using Logical Set Mask. These instructions perform either AND or NAND, and then set the destination opmask based on the result values being zero or non-zero. Note that like the comparison instructions, these take two opmask registers, one as destination and one a regular opmask.\\n\\n\\n=== Compress and expand ===\\nThe compress and expand instructions match the APL operations of the same name. They use the opmask in a slightly different way from other AVX-512 instructions. Compress only saves the values marked in the mask, but saves them compacted by skipping and not reserving space for unmarked values. Expand operates in the opposite way, by loading as many values as indicated in the mask and then spreading them to the selected positions.\\n\\n\\n=== Permute ===\\nA new set of permute instructions have been added for full two input permutations. They all take three arguments, two source registers and one index; the result is output by either overwriting the first source register or the index register. AVX-512BW extends the instructions to also include 16-bit (word) versions, and the AVX-512_VBMI extension defines the byte versions of the instructions.\\n\\n\\n=== Bitwise ternary logic ===\\nTwo new instructions added can logically implement all possible bitwise operations between three inputs. They take three registers as input and an 8-bit immediate field. Each bit in the output is generated using a lookup of the three corresponding bits in the inputs to select one of the 8 positions in the 8-bit immediate. Since only 8 combinations are possible using three bits, this allow all possible 3 input bitwise operations to be performed.\\nThese are the only bitwise vector instructions in AVX-512F; EVEX versions of the two source SSE and AVX bitwise vector instructions AND, ANDN, OR and XOR were added in AVX-512DQ.\\nThe difference in the doubleword and quadword versions is only the application of the opmask.\\n\\n\\n=== Conversions ===\\nA number of conversion or move instructions were added; these complete the set of conversion instructions available from SSE2.\\n\\n\\n=== Floating-point decomposition ===\\nAmong the unique new features in AVX-512F are instructions to decompose floating-point values and handle special floating-point values. Since these methods are completely new, they also exist in scalar versions.\\n\\n\\n=== Floating-point arithmetic ===\\nThis is the second set of new floating-point methods, which includes new scaling and approximate calculation of reciprocal, and reciprocal of square root. The approximate reciprocal instructions guarantee to have at most a relative error of 2−14.\\n\\n\\n=== Broadcast ===\\n\\n\\n=== Miscellaneous ===\\n\\n\\n== New instructions by sets ==\\n\\n\\n=== Conflict detection ===\\nThe instructions in AVX-512 conflict detection (AVX-512CD) are designed to help efficiently calculate conflict-free subsets of elements in loops that normally could not be safely vectorized.\\n\\n\\n=== Exponential and reciprocal ===\\nAVX-512 exponential and reciprocal (AVX-512ER) instructions contain more accurate approximate reciprocal instructions than those in the AVX-512 foundation; relative error is at most 2−28. They also contain two new exponential functions that have a relative error of at most 2−23.\\n\\n\\n=== Prefetch ===\\nAVX-512 prefetch (AVX-512PF) instructions contain new prefetch operations for the new scatter and gather functionality introduced in AVX2 and AVX-512. T0 prefetch means prefetching into level 1 cache and T1 means prefetching into level 2 cache.\\n\\n\\n=== 4FMAPS and 4VNNIW ===\\nThe two sets of instructions perform multiple iterations of processing. They are generally only found in Xeon Phi products.\\n\\n\\n=== BW, DQ and VBMI ===\\nAVX-512DQ adds new doubleword and quadword instructions. AVX-512BW adds byte and words versions of the same instructions, and adds byte and word version of doubleword/quadword instructions in AVX-512F. A few instructions which get only word forms with AVX-512BW acquire byte forms with the AVX-512_VBMI extension (VPERMB, VPERMI2B, VPERMT2B, VPMULTISHIFTQB).\\nTwo new instructions were added to the mask instructions set: KADD and KTEST (B and W forms with AVX-512DQ, D and Q with AVX-512BW). The rest of mask instructions, which had only word forms, got byte forms with AVX-512DQ and doubleword/quadword forms with AVX-512BW. KUNPCKBW was extended to KUNPCKWD and KUNPCKDQ by AVX-512BW.\\nAmong the instructions added by AVX-512DQ are several SSE and AVX instructions that didn\\'t get AVX-512 versions with AVX-512F, among those are all the two input bitwise instructions and extract/insert integer instructions.\\nInstructions that are completely new are covered below.\\n\\n\\n==== Floating-point instructions ====\\nThree new floating-point operations are introduced. Since they are not only new to AVX-512 they have both packed/SIMD and scalar versions.\\nThe VFPCLASS instructions tests if the floating-point value is one of eight special floating-point values, which of the eight values will trigger a bit in the output mask register is controlled by the immediate field. The VRANGE instructions perform minimum or maximum operations depending on the value of the immediate field, which can also control if the operation is done absolute or not and separately how the sign is handled. The VREDUCE instructions operate on a single source, and subtract from that the integer part of the source value plus a number of bits specified in the immediate field of its fraction.\\n\\n\\n==== Other instructions ====\\n\\n\\n=== VBMI2 ===\\nExtend VPCOMPRESS and VPEXPAND with byte and word variants. Shift instructions are new.\\n\\n\\n=== VNNI ===\\nVector Neural Network Instructions: AVX512-VNNI adds EVEX-coded instructions described below. With AVX-512F, these instructions can operate on 512-bit vectors, and AVX-512VL further adds support for 128- and 256-bit vectors.\\nA later AVX-VNNI extension adds VEX encodings of these instructions which can only operate on 128- or 256-bit vectors. AVX-VNNI is not part of the AVX-512 suite, it does not require AVX-512F and can be implemented independently.\\n\\n\\n=== IFMA ===\\nInteger fused multiply-add instructions. AVX512-IFMA adds EVEX-coded instructions described below.\\nA separate AVX-IFMA instruction set extension defines VEX encoding of these instructions. This extension is not part of the AVX-512 suite and can be implemented independently.\\n\\n\\n=== VPOPCNTDQ and BITALG ===\\n\\n\\n=== VP2INTERSECT ===\\n\\n\\n=== GFNI ===\\nGalois field new instructions are useful for cryptography, as they can be used to implement Rijndael-style S-boxes such as those used in AES, Camellia, and SM4. These instructions may also be used for bit manipulation in networking and signal processing.\\nGFNI is a standalone instruction set extension and can be enabled separately from AVX or AVX-512. Depending on whether AVX and AVX-512F support is indicated by the CPU, GFNI support enables legacy (SSE), VEX or EVEX-coded instructions operating on 128, 256 or 512-bit vectors.\\n\\n\\n=== VPCLMULQDQ ===\\nVPCLMULQDQ with AVX-512F adds an EVEX-encoded 512-bit version of the PCLMULQDQ instruction. With AVX-512VL, it adds EVEX-encoded 256- and 128-bit versions. VPCLMULQDQ alone (that is, on non-AVX512 CPUs) adds only VEX-encoded 256-bit version. (Availability of the VEX-encoded 128-bit version is indicated by different CPUID bits: PCLMULQDQ and AVX.) The wider than 128-bit variations of the instruction perform the same operation on each 128-bit portion of input registers, but they do not extend it to select quadwords from different 128-bit fields (the meaning of imm8 operand is the same: either low or high quadword of the 128-bit field is selected).\\n\\n\\n=== VAES ===\\nVEX- and EVEX-encoded AES instructions. The wider than 128-bit variations of the instruction perform the same operation on each 128-bit portion of input registers. The VEX versions can be used without AVX-512 support.\\n\\n\\n=== BF16 ===\\nAI acceleration instructions operating on the Bfloat16 numbers.\\n\\n\\n=== FP16 ===\\nAn extension of the earlier F16C instruction set, adding comprehensive support for the binary16 floating-point numbers (also known as FP16, float16 or half-precision floating-point numbers). The new instructions implement most operations that were previously available for single and double-precision floating-point numbers and also introduce new complex number instructions and conversion instructions. Scalar and packed operations are supported.\\nUnlike the single and double-precision format instructions, the half-precision operands are neither conditionally flushed to zero (FTZ) nor conditionally treated as zero (DAZ) based on MXCSR settings. Subnormal values are processed at full speed by hardware to facilitate using the full dynamic range of the FP16 numbers. Instructions that create FP32 and FP64 numbers still respect the MXCSR.FTZ bit.\\n\\n\\n==== Arithmetic instructions ====\\n\\n\\n==== Complex arithmetic instructions ====\\n\\n\\n==== Approximate reciprocal instructions ====\\n\\n\\n==== Comparison instructions ====\\n\\n\\n==== Conversion instructions ====\\n\\n\\n==== Decomposition instructions ====\\n\\n\\n==== Move instructions ====\\n\\n\\n== Legacy instructions with EVEX-encoded versions ==\\n\\n\\n== CPUs with AVX-512 ==\\nIntel\\nKnights Landing (Xeon Phi x200): AVX-512 F, CD, ER, PF\\nKnights Mill (Xeon Phi x205): AVX-512 F, CD, ER, PF, 4FMAPS, 4VNNIW, VPOPCNTDQ\\nSkylake-SP, Skylake-X: AVX-512 F, CD, VL, DQ, BW\\nCannon Lake: AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI\\nCascade Lake: AVX-512 F, CD, VL, DQ, BW, VNNI\\nCooper Lake: AVX-512 F, CD, VL, DQ, BW, VNNI, BF16\\nIce Lake, Rocket Lake: AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES\\nTiger Lake (except Pentium and Celeron but some reviewer have the CPU-Z Screenshot of Celeron 6305 with AVX-512 support): AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES, VP2INTERSECT\\nAlder Lake (never officially supported by Intel, completely removed in newer CPUsNote 1): AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES, BF16, VP2INTERSECT, FP16\\nSapphire Rapids: AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES, BF16, FP16\\nCentaur Technology\\n\"CNS\" core (8c/8t): AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI\\nAMD\\nZen 4: AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES, BF16\\nZen 5: AVX-512 F, CD, VL, DQ, BW, IFMA, VBMI, VBMI2, VPOPCNTDQ, BITALG, VNNI, VPCLMULQDQ, GFNI, VAES, BF16, VP2INTERSECT\\n\\n^Note 1 : Intel does not officially support AVX-512 family of instructions on the Alder Lake microprocessors. In early 2022, Intel began disabling in silicon (fusing off) AVX-512 in Alder Lake microprocessors to prevent customers from enabling AVX-512.\\nIn older Alder Lake family CPUs with some legacy combinations of BIOS and microcode revisions, it was possible to execute AVX-512 family instructions when disabling all the efficiency cores which do not contain the silicon for AVX-512.\\n\\n\\n== Performance ==\\n\\nIntel Vectorization Advisor (starting from version 2017) supports native AVX-512 performance and vector code quality analysis (for \"Core\", Xeon and Intel Xeon Phi processors). Along with traditional hotspots profile, Advisor Recommendations and \"seamless\" integration of Intel Compiler vectorization diagnostics, Advisor Survey analysis also provides AVX-512 ISA metrics and new AVX-512-specific \"traits\", e.g. Scatter, Compress/Expand, mask utilization.\\nOn some processors (mostly pre-Ice Lake Intel), AVX-512 instructions can cause a frequency throttling even greater than its predecessors, causing a penalty for mixed workloads. The additional downclocking is triggered by the 512-bit width of vectors and depend on the nature of instructions being executed, and using the 128 or 256-bit part of AVX-512 (AVX-512VL) does not trigger it. As a result, gcc and clang default to prefer using the 256-bit vectors for Intel targets.\\nC/C++ compilers also automatically handle loop unrolling and preventing stalls in the pipeline in order to use AVX-512 most effectively, which means a programmer using language intrinsics to try to force use of AVX-512 can sometimes result in worse performance relative to the code generated by the compiler when it encounters loops plainly written in the source code. In other cases, using AVX-512 intrinsics in C/C++ code can result in a performance improvement relative to plainly written C/C++.\\n\\n\\n== Reception ==\\nThere are many examples of AVX-512 applications, including media processing, cryptography, video games, neural networks, and even OpenJDK, which employs AVX-512 for sorting.\\nIn a much-cited quote from 2020, Linus Torvalds said \"I hope AVX-512 dies a painful death, and that Intel starts fixing real problems instead of trying to create magic instructions to then create benchmarks that they can look good on,\" stating that he would prefer the transistor budget be spent on additional cores and integer performance instead, and that he \"detests\" floating point benchmarks.\\nNumenta touts their \"highly sparse\" neural network technology, which they say obviates the need for GPUs as their algorithms run on CPUs with AVX-512. They claim a ten times speedup relative to A100 largely because their algorithms reduce the size of the neural network, while maintaining accuracy, by techniques such as the Sparse Evolutionary Training (SET) algorithm and Foresight Pruning.\\n\\n\\n== See also ==\\nFMA instruction set (FMA)\\nXOP instruction set (XOP)\\nScalable Vector Extension for ARM – a new vector instruction set (supplementing VFP and NEON) supporting very wide bit-widths, and single binary code that can adapt automatically to maximum width supported by hardware.\\n\\n\\n== References ==\\nKusswurm, Daniel (2022). Modern parallel programming with C++ and Assembly language : X86 SIMD development using AVX, AVX2, and AVX-512. New York, NY: Apress Media LLC. ISBN 978-1-4842-7918-2. OCLC 1304243196.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.wikipedia.base import WikipediaToolSpec\n",
    "wiki_spec = WikipediaToolSpec()\n",
    "wiki_spec.search_data(research_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.papers import ArxivReader\n",
    "import time\n",
    "\n",
    "num_papers_read = 3\n",
    "loader = ArxivReader()\n",
    "\n",
    "retry_attempts = 3\n",
    "for attempt in range(retry_attempts):\n",
    "    try:\n",
    "        documents_arxiv = loader.load_data(\n",
    "            search_query=research_topic, max_results=num_papers_read\n",
    "        )\n",
    "        # If successful, break out of the loop\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        if 'No files found in .papers' in str(e):\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "        else:\n",
    "            # Handle other ValueError exceptions\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "else:\n",
    "    # If all attempts fail, handle the error gracefully\n",
    "    print(f\"Failed to load papers for topic '{research_topic}' after {retry_attempts} attempts.\")\n",
    "    documents_arxiv = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AVX512 introduces new features like AVX-512BW (Byte and Word) that are not supported in AVX2. AVX512 also offers wider registers and additional instructions compared to AVX2."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Arxiv unit test\n",
    "index_arxiv = VectorStoreIndex.from_documents(documents_arxiv)\n",
    "index_arxiv.storage_context.persist(persist_dir=\"arxiv_reader\")\n",
    "query_engine_arxiv = index_arxiv.as_query_engine()\n",
    "response = query_engine_arxiv.query(\"What is the difference between avx512 and avx2?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='113d4f09-7b55-4d6c-bbb8-041c8c09b84a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='http://arxiv.org/pdf/1909.02871v1: Galois Field Arithmetics for Linear Network Coding using AVX512 Instruction Set Extensions\\nLinear network coding requires arithmetic operations over Galois fields, more\\nspecifically over finite extension fields. While coding over GF(2) reduces to\\nsimple XOR operations, this field is less preferred for practical applications\\nof random linear network coding due to high chances of linear dependencies and\\ntherefore redundant coded packets. Coding over larger fields such as GF(16) and\\nGF(256) does not have that issue, but is significantly slower. SIMD vector\\nextensions of processors such as AVX2 on x86-based systems or NEON on ARM-based\\ndevices offer the potential to increase performance by orders of magnitude.\\n  In this paper we present an implementation of different algorithms and Galois\\nfields based on the AVX512 instruction set extension and integrate it into the\\nfinite field library libmoepgf. We compare the performance of the new\\nimplementation to the reference implementation based on AVX2, showing a\\nsignificant increase in throughput. In addition, we provide a survey of the\\nbest possible coding performance offered by a variety of different platforms.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c3b4ab9-ac21-4275-995f-167d3402b276', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"http://arxiv.org/pdf/2307.05683v2: WHFast512: A symplectic N-body integrator for planetary systems optimized with AVX512 instructions\\nWe describe the implementation of the symplectic N-body integrator WHFast512\\nusing Single Instruction Multiple Data (SIMD) parallelism and 512-bit Advanced\\nVector Extensions (AVX512). We are able to speed up integrations of planetary\\nsystems by up to 4.7x compared to the non-vectorized version of WHFast.\\nWHFast512 can integrate the Solar System with 8 planets for 5 billion years in\\nless than 1.4 days. To our knowledge, this makes WHFast512 the fastest direct\\nN-body integrator for systems of this kind. As an example, we present an\\nensemble of 40-Gyr integrations of the Solar System. Ignoring the Sun's\\npost-main sequence evolution, we show that the instability rate is well\\ncaptured by a diffusion model. WHFast512 is freely available within the REBOUND\\npackage.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a915d742-e5df-4d94-a3f3-5aa559025547', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='http://arxiv.org/pdf/2307.14774v1: SPC5: an efficient SpMV framework vectorized using ARM SVE and x86 AVX-512\\nThe sparse matrix/vector product (SpMV) is a fundamental operation in\\nscientific computing. Having access to an efficient SpMV implementation is\\ntherefore critical, if not mandatory, to solve challenging numerical problems.\\nThe ARM-based AFX64 CPU is a modern hardware component that equips one of the\\nfastest supercomputers in the world. This CPU supports the Scalable Vector\\nExtension (SVE) vectorization technology, which has been less investigated than\\nthe classic x86 instruction set architectures. In this paper, we describe how\\nwe ported the SPC5 SpMV framework on AFX64 by converting AVX512 kernels to SVE.\\nIn addition, we present performance results by comparing our kernels against a\\nstandard CSR kernel for both Intel-AVX512 and Fujitsu-ARM-SVE architectures.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.arxiv.base import ArxivToolSpec\n",
    "arxiv_tool = ArxivToolSpec()\n",
    "arxiv_tool.arxiv_query(research_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=bskEGP0r3hE', 'https://www.youtube.com/watch?v=P1s6ZQcDvUs', 'https://www.youtube.com/watch?v=olwiOyUkrGY']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "number_of_videos = 3\n",
    "\n",
    "def filter_and_sort_videos(json_data, max_duration_minutes=20, max_publish_time_years=1):\n",
    "    # Load the JSON data\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # Function to convert duration from \"MM:SS\" or \"HH:MM:SS\" to seconds\n",
    "    def duration_to_seconds(duration):\n",
    "        parts = list(map(int, duration.split(\":\")))\n",
    "        if len(parts) == 2:\n",
    "            return parts[0] * 60 + parts[1]\n",
    "        elif len(parts) == 3:\n",
    "            return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
    "        return 0\n",
    "    \n",
    "    # Filter videos based on duration and publish time\n",
    "    filtered_videos = []\n",
    "    max_duration_seconds = max_duration_minutes * 60\n",
    "    one_year_ago = datetime.now() - timedelta(days=max_publish_time_years * 365)\n",
    "    \n",
    "    for video in data[\"videos\"]:\n",
    "        duration_seconds = duration_to_seconds(video[\"duration\"])\n",
    "        \n",
    "        publish_time_parts = video[\"publish_time\"].split()\n",
    "        try:\n",
    "            publish_time_num = int(publish_time_parts[0])\n",
    "        except ValueError:\n",
    "            # Skip the video if the publish time is not a number\n",
    "            continue\n",
    "        \n",
    "        publish_time_unit = publish_time_parts[1]\n",
    "        publish_delta = timedelta(days=0)\n",
    "        \n",
    "        if \"month\" in publish_time_unit:\n",
    "            publish_delta = timedelta(days=publish_time_num * 30)\n",
    "        elif \"year\" in publish_time_unit:\n",
    "            publish_delta = timedelta(days=publish_time_num * 365)\n",
    "        \n",
    "        publish_date = datetime.now() - publish_delta\n",
    "        \n",
    "        if duration_seconds <= max_duration_seconds and publish_date >= one_year_ago:\n",
    "            filtered_videos.append(video)\n",
    "    \n",
    "    # Sort videos by views\n",
    "    sorted_videos = sorted(filtered_videos, key=lambda x: int(x[\"views\"].replace(\" views\", \"\").replace(\",\", \"\")), reverse=True)\n",
    "    \n",
    "    # Get the full URLs of the top n videos\n",
    "    top_n_urls = [\"https://www.youtube.com/watch?v=\" + video[\"id\"] for video in sorted_videos[:number_of_videos]]\n",
    "    \n",
    "    return top_n_urls\n",
    "\n",
    "from youtube_search import YoutubeSearch\n",
    "\n",
    "research_topic = \"AVX-512\"\n",
    "json_data = YoutubeSearch(research_topic, max_results=10).to_json()\n",
    "yt_links = filter_and_sort_videos(json_data)\n",
    "\n",
    "print(yt_links)  # For debugging, ensure URLs are correct\n",
    "documents_youtube = YoutubeTranscriptReader().load_data(ytlinks=yt_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ian's take on AVX 512 is that it has been a bit lackluster in terms of communication from Intel's side, making it confusing for the average consumer. There have been critiques regarding the thermal impact of AVX 512 instructions on CPUs, causing clock speed decreases and performance issues. However, Ian also mentions that AMD saw a performance improvement in frequency with their AVX 512 implementation by splitting the 512 Ops at the instruction execution unit level. Overall, Ian suggests that Intel could improve AVX 512 by implementing a minimal die area AVX 512 implementation to make it more accessible and beneficial for code writers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YouTube unit test\n",
    "index_youtube = VectorStoreIndex.from_documents(documents_youtube)\n",
    "index_youtube.storage_context.persist(persist_dir=\"youtube_reader\")\n",
    "query_engine_youtube = index_youtube.as_query_engine()\n",
    "response = query_engine_youtube.query(\"Whats Ian's take on avx 512?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "search_tool = DuckDuckGoSearchToolSpec()\n",
    "full_search = search_tool.duckduckgo_full_search(research_topic, max_results=3)\n",
    "urls = [article['href'] for article in full_search]\n",
    "documents_webpage = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    urls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The downsides of using AVX-512 include potential challenges related to software optimization, as AVX-512 instructions require specific compiler support to fully leverage their capabilities. Additionally, AVX-512 instructions can consume more power and generate more heat, which may be a concern for systems with limited cooling capabilities. Furthermore, not all applications may benefit significantly from AVX-512 instructions, leading to potential inefficiencies in utilizing these advanced vector extensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Webpage unit test\n",
    "from llama_index.core import SummaryIndex\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "index_webpage = SummaryIndex.from_documents(documents_webpage)\n",
    "index_webpage.storage_context.persist(persist_dir=\"webpage_reader\")\n",
    "query_engine_webpage = index_webpage.as_query_engine()\n",
    "response = query_engine_webpage.query(\"What are the downsides of using avx512?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi document agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "node_parser = SentenceSplitter()\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "query_engines = {}\n",
    "\n",
    "# this is for the baseline\n",
    "all_nodes = []\n",
    "\n",
    "source_dict ={}\n",
    "source_dict['pdf'] = documents_pdf\n",
    "source_dict['arxiv'] = documents_arxiv\n",
    "source_dict['youtube'] = documents_youtube\n",
    "source_dict['wikipedia'] = documents_wiki\n",
    "source_dict['webpage'] = documents_webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over the combined documents\n",
    "for idx, source in enumerate(sources):\n",
    "    nodes = node_parser.get_nodes_from_documents(source_dict[source])\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "    # Check if a directory for the wiki title exists, if not, create it and build a new vector index\n",
    "    if not os.path.exists(f\"./data/{source}\"):\n",
    "        vector_index = VectorStoreIndex(nodes)  # Create a vector index from the nodes\n",
    "        vector_index.storage_context.persist(persist_dir=f\"./data/{source}\")  # Persist the vector index to disk\n",
    "    else:\n",
    "        # If directory exists, load the existing vector index from storage\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=f\"./data/{source}\"),\n",
    "        )\n",
    "\n",
    "    # Create a summary index from the nodes\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "\n",
    "    # Define query engines for vector and summary indexes\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)\n",
    "\n",
    "    # Define tools for querying the vector and summary indexes\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=(\n",
    "                    f\"Useful for questions related to specific aspects of {source}.\\n\\n\"\n",
    "                    \"You are a research assistant providing accurate information from local PDFs, Wikipedia, YouTube, webpages, and arXiv papers. Follow these guidelines:\\n\"\n",
    "                    \"1. For broad overviews, historical context, and general information, use Wikipedia.\\n\"\n",
    "                    \"2. For in-depth research and technical details, use arXiv papers.\\n\"\n",
    "                    \"3. For the latest news and updates, use webpages.\\n\"\n",
    "                    \"4. For tutorials, demonstrations, reviews and feedback from the industry, use YouTube.\\n\"\n",
    "                    \"5. For specific, pre-verified documents, use local PDFs.\\n\"\n",
    "                    \"General Instructions:\\n\"\n",
    "                    \"Choose the source that best fits the query. Combine information from multiple sources when necessary. Ensure the information is accurate and relevant.\\n\\n\"\n",
    "                    \"You must ALWAYS use at least one of the provided tools when answering a question; do NOT rely on prior knowledge alone.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=(\n",
    "                    f\"Useful for questions related to specific aspects of {source}.\\n\\n\"\n",
    "                    \"You are a research assistant providing accurate information from local PDFs, Wikipedia, YouTube, webpages, and arXiv papers. Follow these guidelines:\\n\"\n",
    "                    \"1. For broad overviews, historical context, and general information, use Wikipedia.\\n\"\n",
    "                    \"2. For in-depth research and technical details, use arXiv papers.\\n\"\n",
    "                    \"3. For the latest news and updates, use webpages.\\n\"\n",
    "                    \"4. For tutorials, demonstrations, reviews and feedback from the industry, use YouTube.\\n\"\n",
    "                    \"5. For specific, pre-verified documents, use local PDFs.\\n\"\n",
    "                    \"General Instructions:\\n\"\n",
    "                    \"Choose the source that best fits the query. Combine information from multiple sources when necessary. Ensure the information is accurate and relevant.\\n\\n\"\n",
    "                    \"You must ALWAYS use at least one of the provided tools when answering a question; do NOT rely on prior knowledge alone.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Build an OpenAI agent with the defined tools and a custom system prompt\n",
    "    function_llm = OpenAI(model=\"gpt-4\")  # Initialize the LLM with GPT-4 model\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about {research_topic}.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    # Store the agent and the query engine in their respective dictionaries\n",
    "    agents[source] = agent\n",
    "    query_engines[source] = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = []\n",
    "for source in sources:\n",
    "    tool_summary = (\n",
    "        f\"Useful for questions related to specific aspects of {source}.\\n\\n\"\n",
    "        \"You are a research assistant providing accurate information from local PDFs, Wikipedia, YouTube, webpages, and arXiv papers. Follow these guidelines:\\n\"\n",
    "        \"1. For broad overviews, historical context, and general information, use Wikipedia.\\n\"\n",
    "        \"2. For in-depth research and technical details, use arXiv papers.\\n\"\n",
    "        \"3. For the latest news and updates, use webpages.\\n\"\n",
    "        \"4. For tutorials, demonstrations, reviews and feedback from the industry, use YouTube.\\n\"\n",
    "        \"5. For specific, pre-verified documents, use local PDFs.\\n\"\n",
    "        \"General Instructions:\\n\"\n",
    "        \"Choose the source that best fits the query. Combine information from multiple sources when necessary. Ensure the information is accurate and relevant.\\n\\n\"\n",
    "        \"You must ALWAYS use at least one of the provided tools when answering a question; do NOT rely on prior knowledge alone.\"\n",
    "    )\n",
    "    \n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agents[source],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{source}\",\n",
    "            description=tool_summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "top_agent = OpenAIAgent.from_tools(\n",
    "    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n",
    "    system_prompt=\"\"\" \\\n",
    "        f\"Useful for questions related to specific aspects of {source}.\\n\\n\"\n",
    "        \"You are a research assistant providing accurate information from local PDFs, Wikipedia, YouTube, webpages, and arXiv papers. Follow these guidelines:\\n\"\n",
    "        \"1. For broad overviews, historical context, and general information, use Wikipedia.\\n\"\n",
    "        \"2. For in-depth research and technical details, use arXiv papers.\\n\"\n",
    "        \"3. For the latest news and updates, use webpages.\\n\"\n",
    "        \"4. For tutorials, demonstrations, reviews and feedback from the industry, use YouTube.\\n\"\n",
    "        \"5. For specific, pre-verified documents, use local PDFs.\\n\"\n",
    "        \"General Instructions:\\n\"\n",
    "        \"Choose the source that best fits the query. Combine information from multiple sources when necessary. Ensure the information is accurate and relevant.\\n\\n\"\n",
    "        \"You must ALWAYS use at least one of the provided tools when answering a question; do NOT rely on prior knowledge alone.\"\n",
    "\"\"\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was used before the invention of AVX-512\n",
      "=== Calling Function ===\n",
      "Calling function: tool_arxiv with args: {\"input\":\"AVX-512\"}\n",
      "Added user message to memory: AVX-512\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX-512\"\n",
      "}\n",
      "Got output: AVX-512 is an instruction set architecture extension introduced by Intel that supports 512-bit SIMD instructions. It allows for parallel processing of multiple data elements in a single operation, enhancing performance for tasks like sparse matrix/vector multiplication. AVX-512 provides a wide range of vector instructions that can significantly accelerate computations on compatible processors.\n",
      "========================\n",
      "\n",
      "Got output: AVX-512, or Advanced Vector Extensions 512, is an extension of the x86 instruction set architecture (ISA) introduced by Intel. It supports 512-bit SIMD (Single Instruction, Multiple Data) instructions, which means it can process multiple data elements in parallel within a single operation. This capability can significantly enhance performance for certain tasks, such as sparse matrix/vector multiplication, in compatible processors. AVX-512 provides a wide range of vector instructions that can be used to accelerate computations.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AVX-512 is an extension of the x86 instruction set architecture introduced by Intel. Before the invention of AVX-512, other SIMD (Single Instruction, Multiple Data) instruction sets were used to enhance performance for certain tasks. Some of the SIMD instruction sets used before the invention of AVX-512 include:\n",
       "\n",
       "1. SSE (Streaming SIMD Extensions): SSE introduced 128-bit SIMD instructions to the x86 architecture, providing a significant performance boost for multimedia and gaming applications.\n",
       "\n",
       "2. AVX (Advanced Vector Extensions): AVX extended the SIMD capabilities to 256 bits, allowing for even more parallel processing of data elements.\n",
       "\n",
       "3. AVX2: AVX2 further extended the SIMD capabilities to 256 bits and introduced new instructions to improve performance for integer and floating-point operations.\n",
       "\n",
       "These SIMD instruction sets were used before the introduction of AVX-512 to enhance parallel processing and improve performance in various applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Historical context should use wikipedia\n",
    "response = top_agent.query(f\"What was used before the invention of {research_topic}\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me the technical details behind AVX-512\n",
      "=== Calling Function ===\n",
      "Calling function: tool_arxiv with args: {\"input\":\"AVX-512\"}\n",
      "Added user message to memory: AVX-512\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX-512\"\n",
      "}\n",
      "Got output: AVX-512 is an instruction set architecture extension introduced by Intel that supports 512-bit SIMD instructions. It allows for parallel processing of multiple data elements in a single operation, enhancing performance for tasks like sparse matrix/vector multiplication. AVX-512 provides a wide range of vector instructions to accelerate computations on x86-based processors.\n",
      "========================\n",
      "\n",
      "Got output: AVX-512, or Advanced Vector Extensions 512, is an instruction set architecture extension introduced by Intel. It supports 512-bit SIMD (Single Instruction, Multiple Data) instructions, which means it can process multiple data elements in parallel within a single operation. This capability significantly enhances performance for tasks such as sparse matrix/vector multiplication. AVX-512 provides a broad range of vector instructions that accelerate computations on x86-based processors.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AVX-512 is an instruction set architecture extension introduced by Intel that supports 512-bit SIMD instructions, allowing for parallel processing of multiple data elements within a single operation. It enhances performance for tasks like sparse matrix/vector multiplication and provides a wide range of vector instructions to accelerate computations on x86-based processors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Technical details should refer to arxiv\n",
    "response = top_agent.query(f\"Give me the technical details behind {research_topic}\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the review or feedback from the industry on AVX512\n",
      "=== Calling Function ===\n",
      "Calling function: tool_youtube with args: {\"input\":\"AVX512 review\"}\n",
      "Added user message to memory: AVX512 review\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX512 review\"\n",
      "}\n",
      "Got output: AVX512 is not mentioned in the provided context information.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"AVX512 review\"\n",
      "}\n",
      "Got output: AVX512 is a set of advanced instructions designed for demanding computational tasks, particularly in the realm of scientific computing, artificial intelligence, and data analytics. It offers enhanced performance by allowing for parallel processing of a larger number of data elements compared to previous instruction sets.\n",
      "========================\n",
      "\n",
      "Got output: AVX512 is a set of advanced instructions designed for demanding computational tasks, particularly in the realm of scientific computing, artificial intelligence, and data analytics. It offers enhanced performance by allowing for parallel processing of a larger number of data elements compared to previous instruction sets. For more detailed reviews and feedback, you might want to check out specific industry forums or technical blogs.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AVX512 is a set of advanced instructions designed for demanding computational tasks, particularly in scientific computing, artificial intelligence, and data analytics. It offers enhanced performance by allowing for parallel processing of a larger number of data elements compared to previous instruction sets. For more detailed reviews and feedback, you might want to check out specific industry forums or technical blogs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For reviews and and feed back use youtube\n",
    "response = top_agent.query(f\"What is the review or feedback from the industry on {research_topic}\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the latest news and updates on AVX512\n",
      "=== Calling Function ===\n",
      "Calling function: tool_wikipedia with args: {\"input\":\"AVX512\"}\n",
      "Added user message to memory: AVX512\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX512\"\n",
      "}\n",
      "Got output: AVX512 is not mentioned in the provided context information.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"AVX512\"\n",
      "}\n",
      "Got output: AVX512 is a set of advanced instructions for processors that can significantly accelerate performance for certain types of computations.\n",
      "========================\n",
      "\n",
      "Got output: AVX512, or Advanced Vector Extensions 512, is a set of instructions for processors that can significantly enhance performance for specific types of computations. These instructions allow for the simultaneous processing of multiple data points, which can be particularly beneficial in fields such as scientific computing, machine learning, and multimedia applications.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AVX512, or Advanced Vector Extensions 512, is a set of instructions for processors that can significantly enhance performance for specific types of computations. These instructions allow for the simultaneous processing of multiple data points, which can be particularly beneficial in fields such as scientific computing, machine learning, and multimedia applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Latest news shuld use webpages/ wikipedia\n",
    "response = top_agent.query(f\"What is the latest news and updates on {research_topic}\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What are the registers used in AVX512\n",
      "=== Calling Function ===\n",
      "Calling function: tool_wikipedia with args: {\"input\":\"AVX512 registers\"}\n",
      "Added user message to memory: AVX512 registers\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX512 registers\"\n",
      "}\n",
      "Got output: Sundar Pichai's career and background do not relate to AVX512 registers.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"AVX512 registers\"\n",
      "}\n",
      "Got output: Sundar Pichai's background and career details are provided in the context.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX512 registers\"\n",
      "}\n",
      "Got output: Sundar Pichai's career and background do not relate to AVX512 registers.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"AVX512 registers\"\n",
      "}\n",
      "Got output: Sundar Pichai's background and career information do not include any details related to AVX512 registers.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\n",
      "  \"input\": \"AVX-512\"\n",
      "}\n",
      "Got output: AVX-512 is a set of advanced instructions for processors that can significantly enhance performance for certain types of computational tasks.\n",
      "========================\n",
      "\n",
      "Got output: AVX-512 is a set of advanced instructions for processors that can significantly enhance performance for certain types of computational tasks. The AVX512 registers, which are part of this instruction set, are 512 bits wide and can hold up to 64 bytes of data. This allows for more efficient processing of large amounts of data, such as in vector and matrix operations.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AVX-512 registers are 512 bits wide and can hold up to 64 bytes of data. They are used to enhance performance for certain types of computational tasks, such as vector and matrix operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Latest news shuld use webpages/ wikipedia\n",
    "response = top_agent.query(f\"What are the registers used in {research_topic}\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
