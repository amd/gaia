{
    "dev_mode": true,
    "llm_server": true,
    "show_label": true,

    "agents": [
        "Llm",
        "Chaty"
    ],
    "models": {
        "llama-3.2 1b (ollama)": {
            "backend" : "ollama",
            "checkpoint": "llama3.2:1b",
            "device": {
                "cpu": [
                    "Q8_0"
                ]
            }
        },
        "llama-3.2 3b (ollama)": {
            "backend" : "ollama",
            "checkpoint": "llama3.2:3b",
            "device": {
                "cpu": [
                    "Q4_K_M"
                ]
            }
        },
        "llama-3.1 8b (ollama)": {
            "backend" : "ollama",
            "checkpoint": "llama3.1:8b",
            "device": {
                "cpu": [
                    "Q4_0"
                ]
            }
        },
        "Phi 3.5 mini instruct (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Phi-3.5-mini-instruct-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Phi 3 mini instruct (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Phi-3-mini-4k-instruct-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Llama 2 7b (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Llama-2-7b-hf-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Llama 2 7b chat (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Llama2-7b-chat-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Llama 3.1 8b (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Llama-3.1-8B-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Mistral 7b instruct (oga)": {
            "backend" : "oga",
            "checkpoint": "amd/Mistral-7B-Instruct-v0.3-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "Qwen 1.5 7b chat (oga-npu)": {
            "backend" : "oga",
            "checkpoint": "amd/Qwen1.5-7B-Chat-awq-g128-int4-asym-fp32-onnx-ryzen-strix",
            "device": {
                "npu": [
                    "int4"
                ]
            }
        },
        "llama-3 8b (hf)": {
            "backend" : "hf",
            "checkpoint": "meta-llama/Meta-Llama-3-8B",
            "device": {
                "cpu": [
                    "bfloat16"
                ]
            }
        },
        "llama-2 7b (hf)": {
            "backend" : "hf",
            "checkpoint": "meta-llama/Llama-2-7b-chat-hf",
            "device": {
                "cpu": [
                    "bfloat16"
                ]
            }
        },
        "llama-3.1 8b (hf)": {
            "backend" : "hf",
            "checkpoint": "meta-llama/Meta-Llama-3.1-8B",
            "device": {
                "cpu": [
                    "bfloat16"
                ]
            }
        }
    },
    "max_new_tokens": 600,
    "hide_agents": false,
    "hide_models": false,
    "hide_devices": false
}
