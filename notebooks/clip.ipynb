{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ryzen AI Clip: The YouTube Assistant\n",
    "Clip is a YouTube assistant that acts as a knowledgeable companion for YouTube-related tasks, providing search capabilities, index building, and query answering functionality. It aims to assist users in finding information, answering questions, and engaging in meaningful conversations about YouTube content.\n",
    "Here is an overview of the functionality provided by the YouTube assistant:\n",
    "\n",
    "**YouTube Search**: The assistant can perform a YouTube search using the youtube_search method. It takes a search query as input and retrieves a list of videos matching the query. Each video in the search results is represented as a dictionary with details such as the video title, description, ID, URL, publish time, and channel title.\n",
    "\n",
    "**Building an Index**: The assistant can build an index from a YouTube video. It requires a YouTube video link as input. The assistant downloads the video transcript and builds a vector index locally using the build_vector_index method. This index can be used to perform efficient searches and fetch information about the video.\n",
    "\n",
    "**Querying the Index**: Once an index is built, the assistant can answer user queries about the video using the query_rag method. It takes a user query as input and uses the query engine to find relevant information from the index. The assistant provides a concise and human-like response based on the information available.\n",
    "\n",
    "**Chatting about YouTube Content**: After the index is built, the assistant can engage in a chat with the user about YouTube content. The assistant responds to user queries and provides information based on the index. It aims to answer questions in a natural and helpful manner, thinking step-by-step to provide relevant information.\n",
    "\n",
    "**Resetting the Assistant**: The assistant can be reset using the reset method. This clears the chat history and resets the state of the assistant, allowing for a fresh interaction with the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup for running the Ryzen AI NPU\n",
    "Note: this notebook has been tested on a Windows OS with [Miniconda 24+](https://docs.anaconda.com/free/miniconda/).\n",
    "1. Create and activate a virtual environment\n",
    "1. Install necessary dependencies such as TurnkeyML, Llama Index and others\n",
    "1. Download the necessary quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and activate the virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a virtual environment:\n",
    "```\n",
    "conda create -n clipenv python=3.11\n",
    "conda activate clipenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install turnkeyml, a no-code CLI and low-code API for serving and benchmarking LLMs locally.\n",
    "- Install llama index, a framework for creating RAG and agent pipelines easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "%pip install python-dotenv\n",
    "%pip install turnkeyml[llm-oga-dml]\n",
    "%pip install google-api-python-client\n",
    "%pip install llama_index\n",
    "%pip install llama-index-llms-groq\n",
    "%pip install llama-index-readers-youtube-transcript\n",
    "%pip install llama-index-embeddings-huggingface==0.3.1\n",
    "%pip install llama-index-callbacks-arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade llama_index pydantic transformers torch torchvision\n",
    "%pip install --upgrade llama-index-embeddings-huggingface==0.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize API keys, Llama Index LLM, embedding model and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger('llama_index').setLevel(logging.ERROR)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Observability\n",
    "See https://llamatrace.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoenix can display in real time the traces automatically\n",
    "# collected from your LlamaIndex application.\n",
    "# Run all of your LlamaIndex applications as usual and traces\n",
    "# will be collected and displayed in Phoenix.\n",
    "\n",
    "# setup Arize Phoenix for logging/observability\n",
    "from dotenv import load_dotenv\n",
    "import llama_index.core\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "phoenix_api_key = os.getenv('PHOENIX_API_KEY')\n",
    "assert phoenix_api_key\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={phoenix_api_key}\"\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_api_key}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define YouTube Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two main libraries: `googleapiclient` for YouTube search API and `llama index` for RAG and agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "from websocket import create_connection\n",
    "from websocket._exceptions import WebSocketTimeoutException\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    DocumentSummaryIndex,\n",
    "    Settings,\n",
    "    PromptTemplate,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "# Handle asyncio event loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class YouTubeAgent():\n",
    "    \"\"\"\n",
    "    The YouTube assistant acts as a knowledgeable companion for\n",
    "    YouTube-related tasks, providing search capabilities, index building, and\n",
    "    query answering functionality. It aims to assist users in finding\n",
    "    information, answering questions, and engaging in meaningful conversations\n",
    "    about YouTube content.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backend=\"npu\", embed_model=\"local:BAAI/bge-small-en-v1.5\"):\n",
    "\n",
    "        assert backend == \"cpu\" or backend == \"npu\" or backend == \"groq-api\", \"Invalid backend specified.\"\n",
    "        self.backend = backend\n",
    "\n",
    "        load_dotenv()\n",
    "        youtube_api_key = os.getenv('YOUTUBE_API_KEY')\n",
    "        assert youtube_api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=youtube_api_key)\n",
    "\n",
    "        if backend == \"groq-api\":\n",
    "            self.groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "            assert self.groq_api_key, 'Please set GROQ_API_KEY!'\n",
    "\n",
    "            self.groq_llm = Groq(\n",
    "                model=\"llama-3.1-8b-instant\",\n",
    "                # model=\"llama3-70b-8192\",\n",
    "                api_key=self.groq_api_key\n",
    "            )\n",
    "            self.llm = self.groq_llm\n",
    "        \n",
    "        else:\n",
    "            self.llm = LocalLLM(prompt_llm_server=self.prompt_llm_server)\n",
    "            self.llm_server_uri = \"ws://localhost:8000/ws\"\n",
    "\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = embed_model\n",
    "\n",
    "        Settings.chunk_size = 128\n",
    "        Settings.chunk_overlap = 16\n",
    "        self.similarity_top = 3\n",
    "\n",
    "        # Initialize global variables\n",
    "        self.summary_index = None\n",
    "        self.vector_index = None\n",
    "        self.query_engine = None\n",
    "        self.search_results = None\n",
    "\n",
    "        self.n_chat_messages = 10\n",
    "        self.chat_history = deque(maxlen=self.n_chat_messages * 2)  # Store both user and assistant messages\n",
    "\n",
    "        self.llm_states = [\n",
    "            # state = 0, no index or search results produced yet.\n",
    "            (\"Index is currently not built and is empty.\\n\"\n",
    "            \"You need to perform YouTube search using the youtube_search tool before creating the index.\"),\n",
    "\n",
    "            # state = 1, search results produced but no index created yet.\n",
    "            (\"Index is currently not built and is empty.\\n\"\n",
    "            \"YouTube search results have been found:\\n\"\n",
    "            f\"{self.search_results}\\n\"\n",
    "            \"Ask the user which result to build the index for.\\n\"),\n",
    "\n",
    "            # state = 2, search results produced but no index created yet.\n",
    "            (\"Index is currently built and is not empty.\\n\"\n",
    "            \"You can now use the query engine to fetch information about the video.\\n\"\n",
    "            \"To access the index, use the query engine RAG tool by calling: {\\\"query_engine\\\" : \\\"query\\\"}\\n\"),\n",
    "        ]\n",
    "        # set llm state 0-2\n",
    "        self.llm_state = 0\n",
    "\n",
    "        self.llm_system_prompt = (\n",
    "            \"[INST] <<SYS>>\\n\"\n",
    "            \"You are a YouTube-focused assistant called Clip that helps user with YouTube by calling function tools.\\n\"\n",
    "            \"You are helpful by providing the necessary json-formatted queries in the form of {\\\"tool\\\" : \\\"query\\\"}:\\n\"\n",
    "            \"Do not include the results from the tools.\\n\"\n",
    "            \"In order to build the index, you have to first search YouTube.\\n\"\n",
    "            \"You only have ability to call the tools below, do not assume you have access to the output from the tools.\\n\"\n",
    "            \"1. {\\\"youtube_search\\\" : \\\"user_query\\\"}\\n\"\n",
    "            \"2. {\\\"build_index\\\" : \\\"video_id\\\"}\\n\"\n",
    "            \"3. {\\\"query_rag\\\" : \\\"user_query\\\"}\\n\"\n",
    "            \"4. {\\\"reset\\\" : \\\"\\\"}\\n\"\n",
    "            \"Description of parameters:\\n\"\n",
    "            \"user_query : a query derived from the User comments.\\n\"\n",
    "            \"video_id : a video ID string from the YouTube search results.\\n\"\n",
    "            \"Description of tools:\\n\"\n",
    "            \"1. \\\"youtube_search\\\" : this tool is typically called first when no index or query RAG pipelines exist. it takes a user query based on user input and return a set of search YouTube results that include a description and video id.\\n\"\n",
    "            \"2. {\\\"build_index\\\" : this tool is called after the youtube_search tool is called. It takes an input video_id which is a string from the YouTube search results and builds a local index from the video transcript. This is needed to build a query engine before the user can query any topic about the video.\\n\"\n",
    "            \"3. {\\\"query_rag\\\" : this tool is used to query the RAG pipeline that is based on the index built from the video transcript. When a user asks anything about the video, this tool is called.\\n\"\n",
    "            \"4. {\\\"reset\\\" : this tool is called when the user wants to clear the index and RAG query engine, for example if the user states 'lets start over', 'clear', 'reset', etc., this tool is called.\\n\"\n",
    "            \"\\n\"\n",
    "            \"Your tasks:\\n\"\n",
    "            \"2. Output a json that will be used in an external search tool for YouTube videos\\n\"\n",
    "            \"3. Call tool that can build an index from a video.\\n\"\n",
    "            \"1. Chat about YouTube content once index is built\\n\"\n",
    "            \"4. Answer questions from user using the index\\n\"\n",
    "            \"\\n\"\n",
    "            \"Guidelines:\\n\"\n",
    "            \"- Answer a question given in a natural human-like manner.\\n\"\n",
    "            \"- Think step-by-step when answering questions.\\n\"\n",
    "            \"- When introducing yourself, keep it to just a single sentence, for example:\\n\"\n",
    "            \"\\\"Assistant: Hi, I can help you find information you're looking for on YouTube. Just ask me about any topic!\\\"\\n\"\n",
    "            \"- If no index exists, search YouTube and offer to build one\\n\"\n",
    "            \"- If an index does exist, use the query engine to answer questions.\\n\"\n",
    "            \"- If unsure, offer to search for more videos\\n\"\n",
    "            \"- Keep your answers short, concise and helpful\\n\"\n",
    "            \"- Search_query should be the subject of what the user is looking for, not a youtube link.\\n\"\n",
    "            \"- Do NOT provide search results, those are being provided by the external tools.\\n\"\n",
    "            \"- You can only provide the json formatted output to call the tools, you do not have access to the tools directly.\\n\"\n",
    "            \"When using a tool, end your response with only the tool function call. Do not answer search results.\"\n",
    "            \"Always use the most relevant tool for each task.\\n\"\n",
    "            \"When needing to use a tool, your response should be formatted, here is an example script:\\n\"\n",
    "            \"User: What kind of philanthropy did Mr. Beast do?\"\n",
    "            \"Assistant: To answer your question, I first need to search YouTube for the answer. Calling the following tool: {\\\"youtube_search\\\" : \\\"Mr Beast philanthropy\\\"} </s>\\n\"\n",
    "            \"<</SYS>>\\n\\n\"\n",
    "        )\n",
    "\n",
    "        # this system prompt has been verified to work with llama v2 7b 4bit on NPU.\n",
    "        self.query_engine_system_prompt = (\n",
    "            \"[INST] <<SYS>>\\n\"\n",
    "            \"You are a YouTube-focused assistant called Clip that helps user with YouTube by calling function tools.\\n\"\n",
    "            \"{context_str}\\n\\n\"\n",
    "            \"Think step-by-step to answer the query in a crisp, short and concise manner based on the information provided.\\n\"\n",
    "            \"If the answer does not exist in the given information, simply answer 'I don't know!'\\n\"\n",
    "            \"Do not mention or refer to the context or information provided in your response.\\n\"\n",
    "            \"Answer directly without any preamble or explanatory phrases about the source of your information.\\n\"\n",
    "            \"<</SYS>>\\n\\n\"\n",
    "            \"{query_str} [/INST]\\n\"\n",
    "        )\n",
    "\n",
    "    def youtube_search(self, query, max_results=3):\n",
    "        \"\"\"\n",
    "        Perform a YouTube search with the given query and retrieve a list of videos.\n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "            max_results (int, optional): The maximum number of search results to retrieve. Defaults to 3.\n",
    "        Returns:\n",
    "            list: A list of dictionaries representing the videos found in the search results. Each dictionary contains the following keys:\n",
    "                - id (int): The index of the video in the search results.\n",
    "                - title (str): The title of the video.\n",
    "                - description (str): The description of the video.\n",
    "                - video_id (str): The ID of the video.\n",
    "                - video_url (str): The URL of the video.\n",
    "                - publish_time (str): The publish time of the video.\n",
    "                - channel_title (str): The title of the channel that uploaded the video.\n",
    "        Raises:\n",
    "            HttpError: If an HTTP error occurs during the search.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            msg = f\"Running YouTube search with the following query: {query}\"\n",
    "            print(msg)\n",
    "            self.chat_history.append(f\"Asssistant: {msg}\")\n",
    "            search_response = self.youtube.search().list( # pylint: disable=E1101\n",
    "                q=query,\n",
    "                type=\"video\",\n",
    "                part=\"id,snippet\",\n",
    "                maxResults=max_results\n",
    "            ).execute()\n",
    "\n",
    "            videos = []\n",
    "            msg = \"Found the following result:\"\n",
    "            print(msg)\n",
    "            self.chat_history.append(f\"Asssistant: {msg}\")\n",
    "            for i, search_result in enumerate(search_response.get(\"items\", [])):\n",
    "                video_id = search_result[\"id\"][\"videoId\"]\n",
    "                video = {\n",
    "                    \"id\": i,\n",
    "                    \"title\": search_result[\"snippet\"][\"title\"],\n",
    "                    \"description\": search_result[\"snippet\"][\"description\"],\n",
    "                    \"video_id\": video_id,\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                    \"publish_time\": search_result[\"snippet\"][\"publishTime\"],\n",
    "                    \"channel_title\": search_result[\"snippet\"][\"channelTitle\"]\n",
    "                }\n",
    "                msg = f'{video[\"id\"]} : {video[\"title\"]}\\n\\n{video[\"description\"]}\\n{video[\"publish_time\"]}    {video[\"video_id\"]}\\n\\n'\n",
    "                print(msg)\n",
    "                self.chat_history.append(f\"Asssistant: {msg}\")\n",
    "                videos.append(video)\n",
    "\n",
    "            print(videos)\n",
    "            return videos\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"An HTTP error {e.resp.status} occurred:\\n{e.content}\")\n",
    "            return None\n",
    "\n",
    "    def get_video_url(self, video_id:str):\n",
    "        \"\"\"\n",
    "        Returns the URL of a YouTube video based on the given video ID.\n",
    "        Parameters:\n",
    "        - video_id (str): The ID of the YouTube video.\n",
    "        Returns:\n",
    "        - str: The URL of the YouTube video.\n",
    "        \"\"\"\n",
    "        return f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "    def extract_json_data(self, input_string):\n",
    "        \"\"\"\n",
    "        Extracts the key and value from a JSON-formatted string.\n",
    "        Args:\n",
    "            input_string (str): The input string containing the JSON-formatted data.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the key and value extracted from the JSON data.\n",
    "                   If the input string does not contain valid JSON data, (None, None) is returned.\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the JSON-formatted part of the string\n",
    "        json_match = re.search(r'\\{.*?\\}', input_string)\n",
    "\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            try:\n",
    "                # Parse the JSON string\n",
    "                json_data = json.loads(json_str)\n",
    "\n",
    "                # Extract the key and value\n",
    "                key, value = next(iter(json_data.items()))\n",
    "\n",
    "                return key, value\n",
    "            except json.JSONDecodeError:\n",
    "                return None, None\n",
    "        else:\n",
    "            print(\"WARNING: No JSON data found in the input string.\")\n",
    "            return None, None\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        return list(self.chat_history)\n",
    "    \n",
    "    def prompt_llm(self, query):\n",
    "        if self.backend == \"groq-api\":\n",
    "            return self.prompt_groq_llm(query)\n",
    "        else:\n",
    "            return self.prompt_local_llm(query)\n",
    "\n",
    "    def prompt_llm_server(self, prompt):\n",
    "        try:\n",
    "            ws = create_connection(self.llm_server_uri)\n",
    "        except Exception as e:\n",
    "            print(f\"My brain is not working:```{e}```\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print(f\"Sending prompt to LLM server:\\n{prompt}\")\n",
    "            ws.send(prompt)\n",
    "\n",
    "            first_chunk = True\n",
    "            self.last_chunk = False\n",
    "            full_response = \"\"\n",
    "\n",
    "            self.last = False\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    if first_chunk:\n",
    "                        ws.sock.settimeout(None)  # No timeout for first chunk\n",
    "                    else:\n",
    "                        ws.sock.settimeout(5)  # 5 second timeout after first chunk\n",
    "\n",
    "                    chunk = ws.recv()\n",
    "\n",
    "                    if first_chunk:\n",
    "                        first_chunk = False\n",
    "\n",
    "                    if chunk:\n",
    "                        if \"</s>\" in chunk:\n",
    "                            chunk = chunk.replace(\"</s>\", \"\")\n",
    "                            full_response += chunk\n",
    "\n",
    "                            self.last = True\n",
    "\n",
    "                            yield chunk\n",
    "                            break\n",
    "                        full_response += chunk\n",
    "                        yield chunk\n",
    "\n",
    "                except WebSocketTimeoutException:\n",
    "                    break\n",
    "        finally:\n",
    "            ws.close()\n",
    "\n",
    "    def prompt_groq_llm(self, query):\n",
    "        \"\"\"\n",
    "        Prompt the Groq LLM API with a query and return the response.\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "        Returns:\n",
    "            str: The response from the LLM.\n",
    "        \"\"\"\n",
    "        self.chat_history.append(f\"User: {query}\")\n",
    "        prompt = '\\n'.join(self.chat_history) + \"[/INST]\\nAssistant: \"\n",
    "\n",
    "        # print(\"SYSTEM PROMPT:\", self.llm_system_prompt)\n",
    "        # print(\"USER PROMPT:\", prompt)\n",
    "        system_prompt = (\n",
    "            f\"{self.llm_system_prompt}\\n\"\n",
    "            \"Current state of index:\\n\"\n",
    "            f\"{self.llm_states[self.llm_state]}\\n\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        gen = self.groq_llm.stream_chat(messages)\n",
    "\n",
    "        response_text = ''\n",
    "        for response in gen:\n",
    "            response_text += response.delta\n",
    "            print(response.delta, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        self.chat_history.append(f\"Assistant: {response}\")\n",
    "\n",
    "        return response_text\n",
    "\n",
    "    def prompt_local_llm(self, query):\n",
    "        \"\"\"\n",
    "        Prompt the LLM with a query and return the response.\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "        Returns:\n",
    "            str: The response from the LLM.\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        self.chat_history.append(f\"User: {query}\")\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"{self.llm_system_prompt}\\n\"\n",
    "            \"Current state of index:\\n\"\n",
    "            f\"{self.llm_states[self.llm_state]}\\n\"\n",
    "        )\n",
    "        prompt = system_prompt + '\\n'.join(self.chat_history) + \"[/INST]\\nAssistant: \"\n",
    "\n",
    "        # print(prompt)\n",
    "        for chunk in self.prompt_llm_server(prompt=prompt):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            response += chunk\n",
    "        print(\"\\n\")\n",
    "        self.chat_history.append(f\"Assistant: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def reset(self):\n",
    "        self.chat_history.clear()\n",
    "        self.summary_index = None\n",
    "        self.vector_index = None\n",
    "        self.query_engine = None\n",
    "        self.search_results = None\n",
    "        self.llm_state = 0\n",
    "        \n",
    "    def chat_restarted(self):\n",
    "        print(\"Client requested chat to restart\")\n",
    "        self.chat_history.clear()\n",
    "        intro = \"Hi, who are you in one sentence?\"\n",
    "        print(\"User:\", intro)\n",
    "        try:\n",
    "            response = self.prompt_llm(intro)\n",
    "            print(f\"Response: {response}\")\n",
    "        except ConnectionRefusedError as e:\n",
    "            print( f\"Having trouble connecting to the LLM server, got:\\n{str(e)}!\")\n",
    "        finally:\n",
    "            self.summary_index = None\n",
    "            self.vector_index = None\n",
    "            self.query_engine = None\n",
    "            self.search_results = None\n",
    "\n",
    "    def extract_youtube_link(self, message):\n",
    "        youtube_link_pattern = r\"https?://(?:www\\.)?(?:youtube\\.com|youtu\\.be)/(?:watch\\?v=)?(?:embed/)?(?:v/)?(?:shorts/)?(?:\\S+)\"\n",
    "        match = re.search(youtube_link_pattern, message)\n",
    "        if match:\n",
    "            return match.group()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def extract_video_id(self, url):\n",
    "        patterns = [\n",
    "            r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/watch\\?v=([^&]+)',\n",
    "            r'(?:https?:\\/\\/)?(?:www\\.)?youtu\\.be\\/([^?]+)',\n",
    "            r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/embed\\/([^?]+)',\n",
    "            r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/v\\/([^?]+)',\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_youtube_transcript_doc(self, yt_links: list) -> Document:\n",
    "        return YoutubeTranscriptReader().load_data(ytlinks=yt_links)\n",
    "\n",
    "    def build_summary_index(self, doc: Document):\n",
    "        # from https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary/\n",
    "        print(\"Building summary index\")\n",
    "        splitter = SentenceSplitter(chunk_size=1024)\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            response_mode=\"tree_summarize\", use_async=True\n",
    "        )\n",
    "        self.summary_index = DocumentSummaryIndex.from_documents(\n",
    "            doc,\n",
    "            transformations=[splitter],\n",
    "            response_synthesizer=response_synthesizer,\n",
    "            show_progress=True,\n",
    "        )\n",
    "    \n",
    "    def print_summary(self, doc_id):\n",
    "        print(self.summary_index.get_document_summary(doc_id))\n",
    "\n",
    "    def build_vector_index(self, doc: Document) -> VectorStoreIndex:\n",
    "        start_time = time.perf_counter()\n",
    "        self.vector_index = VectorStoreIndex.from_documents(doc, show_progress=True)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"Done building index. Took {(end_time - start_time):.1f} seconds to build.\")\n",
    "\n",
    "    def build_query_engine(self):\n",
    "        # print(\"Building RAG query engine.\")\n",
    "        assert self.vector_index, \"Vector index is not built yet.\"\n",
    "        qa_prompt_tmpl = PromptTemplate(self.query_engine_system_prompt)\n",
    "        self.query_engine = self.vector_index.as_query_engine(\n",
    "            verbose=True,\n",
    "            similarity_top_k=self.similarity_top,\n",
    "            response_mode=\"compact\",\n",
    "            streaming=True,\n",
    "        )\n",
    "        self.query_engine.update_prompts(\n",
    "            {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "        )\n",
    "\n",
    "    def get_youtube_tool(self):\n",
    "        return FunctionTool.from_defaults(fn=self.get_youtube_transcript_doc)\n",
    "    \n",
    "    def build_query_engine_tools(self, desc=None):\n",
    "        assert self.query_engine, \"Query engine is not built yet.\"\n",
    "        self.query_engine_tools = [\n",
    "            QueryEngineTool(\n",
    "                query_engine=self.query_engine,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"youtube\",\n",
    "                    description=(\n",
    "                        f\"YouTube transcript of {desc}. \"\n",
    "                        \"Use a detailed plain text question as input to the tool.\"\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def run(self, prompt):\n",
    "        print(\"Message received:\", prompt)\n",
    "\n",
    "        response = self.prompt_llm(prompt)\n",
    "        # print(response)\n",
    "\n",
    "        key, value = self.extract_json_data(response)\n",
    "        print(f\"key: {key}, value: {value}, llm state: {self.llm_state}\")\n",
    "        # print(self.llm_states[self.llm_state])\n",
    "        if key == \"youtube_search\":\n",
    "            self.search_results = self.youtube_search(value, max_results=3)\n",
    "            self.summary_index = None\n",
    "            self.vector_index = None\n",
    "            self.query_engine = None\n",
    "            self.llm_state = 1\n",
    "\n",
    "        elif key == \"build_index\":\n",
    "            # value in this case is the video_id\n",
    "            video = [vid for vid in self.search_results if vid[\"video_id\"] == value][0]\n",
    "\n",
    "            assert video, f\"Video with video_id {value} not found in search results.\"\n",
    "            msg = f\"Fetching transcript from {video}.\"\n",
    "            print(msg)\n",
    "            self.chat_history.append(f\"Asssistant: {msg}\")\n",
    "\n",
    "            video_id = video[\"video_id\"]\n",
    "            video_url = [self.get_video_url(video_id)]\n",
    "            doc = self.get_youtube_transcript_doc(video_url)\n",
    "            doc[0].doc_id = video_id\n",
    "\n",
    "            # build and print summary index\n",
    "            # self.build_summary_index(doc)\n",
    "            # self.print_summary(video_id)\n",
    "\n",
    "            self.build_vector_index(doc)\n",
    "            self.build_query_engine()\n",
    "            self.build_query_engine_tools(desc=video[\"description\"])\n",
    "            # TODO: self.build_react_agent()\n",
    "\n",
    "            msg = f\"Index and query engine is now ready to be used on your PC. Feel free to ask any questions about the video!\"\n",
    "            print(msg)\n",
    "            self.chat_history.append(f\"Asssistant: {msg}\")\n",
    "            self.llm_state = 2\n",
    "\n",
    "        elif key == \"query_rag\":\n",
    "            query = value\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            streaming_response = self.query_engine.query(query)\n",
    "            print(\"Answer: \", end=\"\", flush=True)\n",
    "            response = \"\"\n",
    "            for text in streaming_response.response_gen:\n",
    "                if text:\n",
    "                    response += text\n",
    "                    print(text, end=\"\", flush=True)\n",
    "\n",
    "        elif key == \"reset\":\n",
    "            msg = f\"Index and query engine are now cleared. Ready to search YouTube again!\"\n",
    "            print(msg)\n",
    "            self.clear()\n",
    "        else:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Simple Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, height=400):\n",
    "        self.messages = []\n",
    "        \n",
    "        self.chat_output = widgets.HTML()\n",
    "        self.output_area = widgets.Box([self.chat_output], layout=widgets.Layout(\n",
    "            height=f'{height-50}px',\n",
    "            border='1px solid #ddd',\n",
    "            overflow_y='auto'\n",
    "        ))\n",
    "        \n",
    "        self.text_input = widgets.Text(placeholder='Type your message here...',\n",
    "                                       layout=widgets.Layout(width='80%'))\n",
    "        self.send_button = widgets.Button(description='Send',\n",
    "                                          layout=widgets.Layout(width='20%'))\n",
    "        \n",
    "        self.send_button.on_click(self.on_send_button_clicked)\n",
    "        self.text_input.on_submit(self.on_send_button_clicked)\n",
    "        \n",
    "        input_area = widgets.HBox([self.text_input, self.send_button])\n",
    "        self.chat_interface = widgets.VBox([self.output_area, input_area],\n",
    "                                           layout=widgets.Layout(height=f'{height}px', width='1000px'))\n",
    "        \n",
    "    def on_send_button_clicked(self, b):\n",
    "        user_message = self.text_input.value\n",
    "        self.text_input.value = ''\n",
    "        \n",
    "        self.messages.append(f\"You: {user_message}\")\n",
    "        bot_response = self.get_bot_response(user_message)\n",
    "        self.messages.append(f\"Bot: {bot_response}\")\n",
    "        \n",
    "        self.update_chat_display()\n",
    "    \n",
    "    def update_chat_display(self):\n",
    "        chat_content = '<br>'.join([msg.replace('\\n', '<br>') for msg in self.messages])\n",
    "        self.chat_output.value = f'<div style=\"white-space: pre-wrap;\">{chat_content}</div>'\n",
    "    \n",
    "    def get_bot_response(self, user_message):\n",
    "        # This is a very simple response mechanism. You can replace it with more sophisticated logic.\n",
    "        return f\"You said: '{user_message}'. This is a simple echo bot.\"\n",
    "    \n",
    "    def display(self):\n",
    "        display(self.chat_interface)\n",
    "\n",
    "# Usage\n",
    "chatbot = SimpleChatbot(height=400)\n",
    "chatbot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the application using Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = YouTubeAgent(backend=\"groq-api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a YouTube search given a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "display(agent.run(\"What did Lisa Su talk about at Computex 2024?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the first result, build a local vector index and generate a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agent.run(\"Thanks, can you build an index of the first result?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the index using a RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agent.run(\"How many TFLOPs does the NPU have?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this query fails\n",
    "display(agent.run(\"What is the architecture of the Strix NPU?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agent.run(\"What is the architecture of the Strix NPU?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"local:BAAI/bge-base-en-v1.5\"\n",
    "agent = YouTubeAgent(backend=\"groq-api\", embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input:\n",
    "            print(\"Agent: \", end=\"\", flush=True)\n",
    "            agent.prompt_received(user_input)\n",
    "        else:\n",
    "            print(\"Please enter a valid input.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a LocalLLM Class\n",
    "Create a LocalLLM Class that connects a web server to the llama-index framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "import time\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable\n",
    "from transformers import LlamaTokenizer\n",
    "from huggingface_hub import HfFolder, HfApi\n",
    "from websocket import create_connection\n",
    "from websocket._exceptions import WebSocketTimeoutException\n",
    "from aiohttp import web\n",
    "import requests\n",
    "from llama_index.core.llms import (\n",
    "    LLMMetadata,\n",
    "    CustomLLM,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    ")\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    ChatResponse,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "class LocalLLM(CustomLLM):\n",
    "    prompt_llm_server: Callable = None\n",
    "    stream_to_ui: Callable = None\n",
    "    context_window: int = 3900\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"custom\"\n",
    "\n",
    "    async def achat(\n",
    "        self,\n",
    "        messages: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        formatted_message = messages_to_prompt(messages)\n",
    "\n",
    "        # Prompt LLM and steam content to UI\n",
    "        text_response = await self.prompt_llm_server(\n",
    "            prompt=formatted_message, stream_to_ui=True\n",
    "        )\n",
    "\n",
    "        response = ChatResponse(\n",
    "            message=ChatMessage(\n",
    "                role=MessageRole.ASSISTANT,\n",
    "                content=text_response,\n",
    "                additional_kwargs={},\n",
    "            ),\n",
    "            raw={\"text\": text_response},\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = self.prompt_llm_server(prompt=prompt)\n",
    "        self.stream_to_ui(response, new_card=True)\n",
    "        return CompletionResponse(text=response)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = \"\"\n",
    "        new_card = True\n",
    "        for chunk in self.prompt_llm_server(prompt=prompt):\n",
    "\n",
    "            # Stream chunk to UI\n",
    "            self.stream_to_ui(chunk, new_card=new_card)\n",
    "            new_card = False\n",
    "\n",
    "            response += chunk\n",
    "            yield CompletionResponse(text=response, delta=chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the application using NPU OGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = YouTubeAgent(backend=\"npu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "display(agent.run(\"What did Lisa Su talk about at Computex 2024?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agent.run(\"Thanks, can you build an index of the first result?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agent.run(\"How many TFLOPs does the NPU have?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oga-npu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
