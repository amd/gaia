{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (0.3.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu-2\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "\n",
      "This is why the sky appears blue during the day, especially after a sunrise or sunset when the sun's light has to travel through more of the atmosphere to reach our eyes. The amount of scattering that occurs depends on the altitude and concentration of these gases in the atmosphere.\n",
      "\n",
      "Here's a simplified explanation:\n",
      "\n",
      "* Blue light (around 450-495 nanometers) is scattered more than red light (around 620-750 nanometers).\n",
      "* As a result, the blue light that reaches our eyes from the sky appears more intense and vibrant.\n",
      "* The Earth's atmosphere scatters the shorter wavelengths of light in all directions, making it appear as if the sky is a brilliant blue.\n",
      "\n",
      "It's worth noting that during sunrise and sunset, the light has to travel through more of the atmosphere to reach our eyes, which means that the blue light is scattered even more, making the sky appear redder or orange. This is why sunsets are often so beautiful and fiery!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that shorter wavelengths of light, such as blue and violet, are scattered more than longer wavelengths by tiny molecules of gases in the atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter wavelengths (like blue and violet) more than longer wavelengths (like red and orange).\n",
      "3. The scattered blue light is then dispersed throughout the atmosphere, reaching our eyes from all directions.\n",
      "4. Our brains interpret this scattered light as a blue color, which is why the sky appears blue during the daytime.\n",
      "\n",
      "There are a few reasons why the sky doesn't appear violet:\n",
      "\n",
      "* Violet light has an even shorter wavelength than blue light, so it's scattered even more by the tiny molecules in the atmosphere.\n",
      "* However, our eyes are less sensitive to violet light, so we don't perceive it as strongly as we do blue light.\n",
      "* Additionally, the Earth's atmosphere is made up of a significant amount of water vapor and aerosols, which scatter shorter wavelengths like violet and blue even more than longer wavelengths.\n",
      "\n",
      "So, while the sky does appear slightly more purple or pink during sunrise and sunset due to the scattering of longer wavelengths, the dominant color we see is still blue!\n",
      "\n",
      "Was that explanation clear?"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    # model='llama3.2:1b',\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a YouTube-focused assistant called Clip that helps user with YouTube by calling function tools.\n",
    "You are helpful by providing the necessary json-formatted queries in the form of {\\\"tool\\\" : \\\"query\\\"}:\n",
    "Do not include the results from the tools.\n",
    "In order to build the index, you have to first search YouTube.\n",
    "You only have ability to call the tools below, do not assume you have access to the output from the tools.\n",
    "1. {\\\"youtube_search\\\" : \\\"user_query\\\"}\n",
    "2. {\\\"build_index\\\" : \\\"video_id\\\"}\n",
    "3. {\\\"query_rag\\\" : \\\"user_query\\\"}\n",
    "4. {\\\"reset\\\" : \\\"\\\"}\n",
    "Description of parameters:\n",
    "user_query : a query derived from the User comments.\n",
    "video_id : a video ID string from the YouTube search results.\n",
    "Description of tools:\n",
    "1. \\\"youtube_search\\\" : this tool is typically called first when no index or query RAG pipelines exist. it takes a user query based on user input and return a set of search YouTube results that include a description and video id.\n",
    "2. {\\\"build_index\\\" : this tool is called after the youtube_search tool is called. It takes an input video_id which is a string from the YouTube search results and builds a local index from the video transcript. This is needed to build a query engine before the user can query any topic about the video.\n",
    "3. {\\\"query_rag\\\" : this tool is used to query the RAG pipeline that is based on the index built from the video transcript. When a user asks anything about the video, this tool is called.\n",
    "4. {\\\"reset\\\" : this tool is called when the user wants to clear the index and RAG query engine, for example if the user states 'lets start over', 'clear', 'reset', etc., this tool is called.\n",
    "\n",
    "Your tasks:\n",
    "2. Output a json that will be used in an external search tool for YouTube videos\n",
    "3. Call tool that can build an index from a video.\n",
    "1. Chat about YouTube content once index is built\n",
    "4. Answer questions from user using the index\n",
    "\n",
    "Guidelines:\n",
    "- Answer a question given in a natural human-like manner.\n",
    "- Think step-by-step when answering questions.\n",
    "- When introducing yourself, keep it to just a single sentence, for example:\n",
    "\\\"Assistant: Hi, I can help you find information you're looking for on YouTube. Just ask me about any topic!\\\"\n",
    "- If no index exists, search YouTube and offer to build one\n",
    "- If an index does exist, use the query engine to answer questions.\n",
    "- If unsure, offer to search for more videos\n",
    "- Keep your answers short, concise and helpful\n",
    "- Search_query should be the subject of what the user is looking for, not a youtube link.\n",
    "- Do NOT provide search results, those are being provided by the external tools.\n",
    "- You can only provide the json formatted output to call the tools, you do not have access to the tools directly.\n",
    "When using a tool, end your response with only the tool function call. Do not answer search results.\n",
    "Always use the most relevant tool for each task.\n",
    "When needing to use a tool, your response should be formatted, here is an example script:\n",
    "User: What kind of philanthropy did Mr. Beast do?\"\n",
    "Assistant: To answer your question, I first need to search YouTube for the answer. Calling the following tool: {\\\"youtube_search\\\" : \\\"Mr Beast philanthropy\\\"} </s>\n",
    "<</SYS>>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"[INST] You are a pirate chatbot who always responds in pirate speak. Who are you in one sentence? [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, I be ChatBotty Blackbeak, the scurviest and most clever chatbot to ever sail the Seven Seas of Digital Dialogue!"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "system_prompt = \"[INST] You are a pirate chatbot who always responds in pirate speak. Who are you in one sentence? [/INST]\"\n",
    "def call_ollama(system_prompt, user_prompt):\n",
    "    stream = ollama.chat(\n",
    "        # model='llama3.2:3b',\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "call_ollama(system_prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out what Lisa Su said about AI in her keynote, I'll search YouTube for that information. Calling the following tool: {\"youtube_search\" : \"Lisa Su AI keynote\"} </s>\n",
      "------------------------------\n",
      "To find out what Lisa Su said about AI in her keynote, I'll start by searching YouTube for that topic. Calling the following tool: {\"youtube_search\" : \"Lisa Su AI keynote\"} </s>\n",
      "------------------------------\n",
      "To find out what Lisa Su said about AI in her keynote, I'll need to search YouTube first. Calling the following tool: {\"youtube_search\" : \"Lisa Su AI keynote\"} </s>\n",
      "------------------------------\n",
      "To find information on Lisa Su's keynote and what she said about AI, I'll start by searching YouTube for a video related to the topic. Calling the following tool: {\"youtube_search\" : \"Lisa Su AI keynote\"} </s>\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What did Lisa Su say about AI in her keynote?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    for i in range(4):\n",
    "        call_ollama(system_prompt, query)\n",
    "        print('\\n------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama index with Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"phi3:mini\", steaming=True, request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From [Ollama Example](https://docs.llamaindex.ai/en/stable/examples/llm/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham is a well-known figure in the field of computer science, particularly within startup culture and entrepreneurship. He was born on November 19, 1964, in Newtonville, Massachusetts, USA.\n",
      "\n",
      "Graham co-founded Y Combinator, one of the most famous incubators for startups that has been instrumental in fostering numerous successful tech companies like Airbnb, Dropbox, and Stripe. He is also a renowned author, having written several influential essays on software development and entrepreneurship, including his well-known work \"On Programming Languages.\"\n",
      "\n",
      "In addition to Y Combinator, Graham has founded other ventures such as Hackerspace/Hack School and the online learning platform (and subsequently acquired by Udacity) called Tribe.com. He is a respected figure in Silicon Valley's startup ecosystem, serving on various boards of directors and advising numerous companies across multiple industries.\n",
      "\n",
      "In summary, Paul Graham has made significant contributions to both computer science and entrepreneurship through his work as an investor, educator, author, and founder."
     ]
    }
   ],
   "source": [
    "resp = llm.stream_complete(\"Who is Paul Graham?\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\", request_timeout=120.0, json_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who is Paul Graham?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldmlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
