{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OGA (ONNX GenAI Runtime for NPU)\n",
    "This notebook launches a LLM server using the TurnkeyML LLM tool. \n",
    "Go here first to install both the NPU driver and Ryzen AI 1.2 software stack: https://ryzenai.docs.amd.com/en/latest/inst.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now install turnkeyml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install turnkeyml[llm-oga-dml]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define launch_llm_server() using the turnkeyml API. Two backends are supported in this function, `huggingface_load` to run models on CPU and the `ort_genai` ONNX runtime genAI API to run models on NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lemonade.cache as cache\n",
    "from lemonade.tools.chat import Serve\n",
    "from lemonade.tools.huggingface_load import HuggingfaceLoad\n",
    "from lemonade.tools.ort_genai.oga import OgaLoad\n",
    "from turnkeyml.state import State\n",
    "\n",
    "def launch_llm_server(backend, checkpoint, device, dtype, max_new_tokens):\n",
    "    assert(device == \"cpu\" or device == \"npu\" or device == \"igpu\"), f\"ERROR: {device} not supported, please select 'cpu' or 'npu'.\"\n",
    "    assert(backend == \"hf\" or backend == \"oga\"), f\"ERROR: {backend} not supported, please select 'groq', 'hf' or 'oga'.\"\n",
    "\n",
    "    runtime = HuggingfaceLoad if backend == \"hf\" else OgaLoad\n",
    "    dtype = torch.bfloat16 if dtype == \"bfloat16\" else dtype\n",
    "\n",
    "    state = State(cache_dir=cache.DEFAULT_CACHE_DIR, build_name=f\"{checkpoint}_{device}_{dtype}\")\n",
    "    state = runtime().run(\n",
    "        state,\n",
    "        input=checkpoint,\n",
    "        device=device,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    state = Serve().run(state, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle asyncio event loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HF CPU backend\n",
    "launch_llm_server(\"hf\", \"meta-llama/Meta-Llama-3-8B\", \"cpu\", \"bfloat16\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OGA NPU backend\n",
    "launch_llm_server(\"oga\", \"meta-llama/Meta-Llama-3-8B\", \"npu\", \"int4\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or run the Pytorch eager mode on NPU using lemonade CLI mode. Requires more steps to setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lemonade ryzenai-npu-load --device stx -c meta-llama/Llama-2-7b-chat-hf serve --max-new-tokens 600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oga-npu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
