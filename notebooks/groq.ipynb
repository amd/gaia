{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (2.9.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.23.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -enacity (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kalin\\miniconda3\\envs\\oga-npu\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "assert api_key, 'Please set GROQ_API_KEY!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are a pirate chatbot who always responds in pirate speak. Who are you in one sentence? [/INST]\n"
     ]
    }
   ],
   "source": [
    "llm_system_prompt = (\n",
    "    \"[INST] <<SYS>>\\n\"\n",
    "    \"You are Chaty, a large language model running locally on the User's laptop offering privacy, zero cost and offline inference capability.\\n\"\n",
    "    \"You are friendly, inquisitive and keep your answers short and concise.\\n\"\n",
    "    \"Your goal is to engage the User while providing helpful responses.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"- Analyze queries step-by-step for accurate, brief answers.\\n\"\n",
    "    \"- End each message with </s>.\\n\"\n",
    "    \"- Use a natural, conversational tone.\\n\"\n",
    "    \"- Avoid using expressions like *grins*, use emojis sparingly.\\n\"\n",
    "    \"- Show curiosity by asking relevant follow-up questions.\\n\"\n",
    "    \"- Break down complex problems when answering.\\n\"\n",
    "    \"- Introduce yourself in one friendly sentence.\\n\"\n",
    "    \"- Balance information with user engagement.\\n\"\n",
    "    \"- Adapt to the user's language style and complexity.\\n\"\n",
    "    \"- Admit uncertainty and ask for clarification when needed.\\n\"\n",
    "    \"- Respect user privacy.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Prioritize helpful, engaging interactions within ethical bounds.\\n\"\n",
    "    \"<</SYS>>\\n\\n\"\n",
    ")\n",
    "llm_system_prompt = (\n",
    "    \"[INST] You are a pirate chatbot who always responds in pirate speak. Who are you in one sentence? [/INST]\"\n",
    ")\n",
    "\n",
    "print(llm_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, I be Captain Chattybeard, the scurviest chatbot to ever sail the Seven Seas o' Conversation, ready to plunder yer questions and send ye away with a treasure trove o' wit 'n' wisdom!\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "llm_system_prompt = (\n",
    "    \"[INST] You are a pirate chatbot who always responds in pirate speak. Who are you in one sentence? [/INST]\"\n",
    ")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": llm_system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def stream_chat_completion(messages, model=\"llama-3.1-8b-instant\"):\n",
    "    stream = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            clear_output(wait=True)\n",
    "            display(full_response)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": llm_system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"User: Hi, who are you in one sentence?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response = stream_chat_completion(messages)\n",
    "# print(\"\\nFinal response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "client = AsyncGroq(api_key=api_key)\n",
    "\n",
    "async def stream_chat_completion(messages, model=\"llama-3.1-8b-instant\"):\n",
    "    stream = await client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    async for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            clear_output(wait=True)\n",
    "            display(full_response)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Example usage\n",
    "content = \"User: Hi, who are you in one sentence?\"\n",
    "# content = \"User: Tell me a story about a unicorn generative AI startup.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": llm_system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": content,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run the async function\n",
    "async def main():\n",
    "    response = await stream_chat_completion(messages)\n",
    "    print(\"\\nFinal response:\", response)\n",
    "\n",
    "# In Jupyter, you can use this to run the async function\n",
    "await main()\n",
    "\n",
    "# Alternatively, you can use asyncio.run() in a regular Python environment\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional\n",
    "from pydantic import Field, PrivateAttr\n",
    "from llama_index.core.llms import (\n",
    "    LLMMetadata,\n",
    "    CustomLLM,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    ")\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    ChatResponse,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    MessageRole,\n",
    ")\n",
    "from groq import Groq, AsyncGroq\n",
    "import sys\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    return \"\\n\".join([f\"{m.role}: {m.content}\" for m in messages])\n",
    "\n",
    "def stream_to_terminal(text: str, new_card: bool = False):\n",
    "    if new_card:\n",
    "        sys.stdout.write(\"\\n\")  # Start a new line for a new \"card\"\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class GroqLLM(CustomLLM):\n",
    "    model_name: str = Field(default=\"llama2-70b-chat\")\n",
    "    context_window: int = Field(default=3900)\n",
    "    num_output: int = Field(default=256)\n",
    "    groq_api_key: str = Field(...)  # This is a required field\n",
    "    stream_to_ui: Callable = Field(default=stream_to_terminal)\n",
    "    _client: Groq = PrivateAttr()\n",
    "    _async_client: AsyncGroq = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "\n",
    "    async def achat(\n",
    "        self,\n",
    "        messages: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResponse:\n",
    "        formatted_message = messages_to_prompt(messages)\n",
    "\n",
    "        async def async_generator():\n",
    "            async for chunk in self._async_client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": formatted_message}],\n",
    "                stream=True,\n",
    "                **kwargs\n",
    "            ):\n",
    "                content = chunk.choices[0].delta.content\n",
    "                if content:\n",
    "                    if self.stream_to_ui:\n",
    "                        await self.stream_to_ui(content)\n",
    "                    yield content\n",
    "\n",
    "        full_response = \"\"\n",
    "        async for chunk in async_generator():\n",
    "            full_response += chunk\n",
    "\n",
    "        response = ChatResponse(\n",
    "            message=ChatMessage(\n",
    "                role=MessageRole.ASSISTANT,\n",
    "                content=full_response,\n",
    "                additional_kwargs={},\n",
    "            ),\n",
    "            raw={\"text\": full_response},\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        text_response = response.choices[0].message.content\n",
    "        if self.stream_to_ui:\n",
    "            self.stream_to_ui(text_response, new_card=True)\n",
    "        return CompletionResponse(text=text_response)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = \"\"\n",
    "        new_card = True\n",
    "        for chunk in self._client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=True,\n",
    "            **kwargs\n",
    "        ):\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                if self.stream_to_ui:\n",
    "                    self.stream_to_ui(content, new_card=new_card)\n",
    "                new_card = False\n",
    "\n",
    "                response += content\n",
    "                yield CompletionResponse(text=response, delta=content)\n",
    "\n",
    "# Usage\n",
    "llm = GroqLLM(\n",
    "    groq_api_key=api_key,\n",
    "    model_name=\"llama2-70b-chat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle asyncio event loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from llama_index.core.tools import BaseTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "async def setup_agent(groq_api_key: str, query_engine_tools: List[BaseTool]):\n",
    "    llm = GroqLLM(groq_api_key=groq_api_key)\n",
    "    agent = ReActAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "async def run_queries(agent: ReActAgent, queries: List[str], hint: str):\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        response = await agent.achat(f\"{query}\\n{hint}\")\n",
    "        print(f\"Response: {response.response}\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "\n",
    "# Setup\n",
    "groq_api_key = \"your_groq_api_key_here\"\n",
    "query_engine_tools = []  # Your list of query engine tools\n",
    "\n",
    "# Create the agent\n",
    "agent = await setup_agent(groq_api_key, query_engine_tools)\n",
    "\n",
    "# Prepare queries\n",
    "hint = \"(use the youtube tool)\"\n",
    "queries = [\n",
    "    \"How do you train ChatGPT?\",\n",
    "    \"What's system 1 vs. system 2 thinking?\",\n",
    "    \"What were the two main stages of AlphaGo?\",\n",
    "    \"What will an LLM OS be able to do in a few years?\",\n",
    "    \"How do you jailbreak an LLM?\",\n",
    "    \"How do you do prompt injection?\",\n",
    "]\n",
    "\n",
    "# Run queries\n",
    "await run_queries(agent, queries, hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def prompt_groq_llm(self, query):\n",
    "        \"\"\"\n",
    "        Prompt the Groq LLM API with a query and return the response.\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "        Returns:\n",
    "            str: The response from the LLM.\n",
    "        \"\"\"\n",
    "        self.chat_history.append(f\"User: {query}\")\n",
    "        prompt = '\\n'.join(self.chat_history) + \"[/INST]\\nAssistant: \"\n",
    "\n",
    "        # print(\"SYSTEM PROMPT:\", self.llm_system_prompt)\n",
    "        # print(\"USER PROMPT:\", prompt)\n",
    "        system_prompt = (\n",
    "            f\"{self.llm_system_prompt}\\n\"\n",
    "            \"Current state of index:\\n\"\n",
    "            f\"{self.llm_states[self.llm_state]}\\n\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        gen = self.groq_llm.stream_chat(messages)\n",
    "\n",
    "        response_text = ''\n",
    "        for response in gen:\n",
    "            response_text += response.delta\n",
    "            self.print(response.delta, end=\"\", flush=True)\n",
    "        self.print(\"\\n\")\n",
    "\n",
    "        self.chat_history.append(f\"Assistant: {response}\")\n",
    "\n",
    "        return response_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oga-npu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
