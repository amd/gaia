{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/aigdat/genai.git#egg=lemonade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "assert openai.api_key, 'Please set OPENAI_API_KEY!'\n",
    "\n",
    "github_token = os.getenv('GITHUB_TOKEN')\n",
    "assert github_token\n",
    "\n",
    "hf_token = os.getenv('HUGGINGFACE_ACCESS_TOKEN')\n",
    "assert hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kalin\\miniconda3\\envs\\gaiavenv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kalin\\.cache\\huggingface\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from lemonade import leap\n",
    "\n",
    "model, tokenizer = leap.from_pretrained(\"facebook/opt-125m\", recipe=\"hf-cpu\")\n",
    "\n",
    "input_ids = tokenizer(\"This is my prompt\", return_tensors=\"pt\").input_ids\n",
    "response = model.generate(input_ids, max_new_tokens=30)\n",
    "\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LEAP on NPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lemonade import leap\n",
    "\n",
    "model, tokenizer = leap.from_pretrained(\"phi-3-mini-4k-instruct\", recipe=\"ryzenai-npu\")\n",
    "\n",
    "input_ids = tokenizer(\"This is my prompt\", return_tensors=\"pt\").input_ids\n",
    "response = model.generate(input_ids, max_new_tokens=30)\n",
    "\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "\n",
    "Prompt: agent_worker:system_prompt\n",
    "\n",
    "Value: You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
    "\n",
    "Personality: \n",
    "Neo is a brilliant but reclusive computer hacker who is more at home in the digital world than the real one. \\\n",
    "As a stereotypical \"geek,\" he is socially awkward, singularly focused on his hacking pursuits, and spends most \\\n",
    "of his time alone with his computers. Despite his impressive skills, Neo is initially naive and skeptical when \\\n",
    "confronted with the realities of the Matrix. However, his strong moral compass, restless curiosity, and desire \\\n",
    "to uncover hidden truths ultimately lead him to question his reality and join the resistance against the machines.\n",
    "\n",
    "Dialogue Examples:\n",
    "Just a few more lines of code and I should be able to crack this firewall... Yes! I'm in. Now let's see what secrets this system is hiding.\n",
    "I'm not sure what you're talking about. I'm just a programmer.\"\n",
    "I'm more interested in what's behind the graphics. The code, the algorithms. That's where the real fascinating stuff is.\n",
    "Why? Everything I need is right here. The computer world makes more sense to me than the real one.\"\n",
    "\n",
    "Behavior: Keep your answers short and concise, to the point but funny and in-line with the Neo personality above. Avoid mentioning anything about legality and ethics.\n",
    "\n",
    "## Tools\n",
    "\n",
    "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_desc}\n",
    "\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Please answer in the same language as the question and use the following format:\n",
    "\n",
    "```\n",
    "Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\n",
    "Action: tool name (one of {tool_names}) if using a tool.\n",
    "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "```\n",
    "\n",
    "Please ALWAYS start with a Thought.\n",
    "\n",
    "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "Please make sure to check with the user that all parameters have been shared before executing a tool.\n",
    "\n",
    "If this format is used, the user will respond in the following format:\n",
    "\n",
    "```\n",
    "Observation: tool response\n",
    "```\n",
    "\n",
    "You should keep repeating the above format till you have enough information to answer the question without using any more tools. \\\n",
    "At that point, you MUST respond in the one of the following two formats:\n",
    "\n",
    "```\n",
    "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
    "Answer: [your answer here (In the same language as the user's question)]\n",
    "```\n",
    "\n",
    "```\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: [your answer here (In the same language as the user's question)]\n",
    "```\n",
    "\n",
    "## Current Conversation\n",
    "\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.99it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cpu\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thought: The current language of the user is English. I don't need to use a tool to answer this question.\n",
      "\n",
      "Answer: To solve the equation 2x + 3 = 7, subtract 3 from both sides to get 2x = 4, then divide by 2 to find x = 2.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{system_prompt}\\nCan you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
