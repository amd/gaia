{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "def count_tokens(text: str, model_name: str = \"gpt2\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        model_name (str, optional): The name of the tokenizer model to use. Defaults to \"gpt2\".\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "file_path: docs\\install.md\n",
    "file_name: docs\\install.md\n",
    "url: https://github.com/onnx\\turnkeyml\\blob/main\\docs\\install.md\n",
    "\n",
    "# TurnkeyML Installation Guide\n",
    "\n",
    "The following describes how to install TurnkeyML.\n",
    "\n",
    "## Operating System Requirements\n",
    "\n",
    "This project is tested using `ubuntu-latest` and `windows-latest`. `ubuntu-latest` currently defaults to Ubuntu 20.04. However, it is also expected to work on other recent versions of Ubuntu (e.g. 18.04, 22.04), and other flavors of Linux (e.g. CentOS).\n",
    "\n",
    "## Installing TurnkeyML\n",
    "\n",
    "### Step 1: Miniconda environment setup\n",
    "\n",
    "We hughly recommend the use of [miniconda](https://docs.conda.io/en/latest/miniconda.html) environments when:\n",
    "\n",
    "If you are installing TurnkeyML on **Linux**, simply run the command below:\n",
    "```\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "```\n",
    "\n",
    "If you are installing TurnkeyML on **Windows**, manually download and install [Miniconda3 for Windows 64-bit](https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe). Please note that PowerShell is recommended when using miniconda on Windows.\n",
    "\n",
    "Then create and activate a virtual environment like this:\n",
    "\n",
    "```\n",
    "conda create -n tkml python=3.8\n",
    "conda activate tkml\n",
    "```\n",
    "\n",
    "### Step 2: Installing TurnkeyML from source\n",
    "\n",
    "First, make sure you have a copy of the repository locally:\n",
    "\n",
    "```\n",
    "git clone https://github.com/onnx/turnkeyml.git\n",
    "```\n",
    "\n",
    "Then, simply pip install the TurnkeyML package:\n",
    "\n",
    "```\n",
    "pip install -e turnkeyml\n",
    "```\n",
    "\n",
    "You are now done installing TurnkeyML!\n",
    "\n",
    "If you are planning to use the `turnkey` tools with the TurnkeyML models or Slurm please see the corresponding sections below.\n",
    "\n",
    "## TurnkeyML Models Requirements\n",
    "\n",
    "The TurnkeyML models are located at `install_path/models`, which we refer to as `models/` in most of the guides.\n",
    "\n",
    "> _Note_: The `turnkey models location` command and `turnkey.common.filesystem.MODELS_DIR` are useful ways to locate the `models` directory. If you perform PyPI installation, we recommend that you take an additional step like this:\n",
    "\n",
    "```\n",
    "(tkml) jfowers:~$ turnkey models location\n",
    "\n",
    "Info: The TurnkeyML models directory is: ~/turnkeyml/models\n",
    "(tkml) jfowers:~$ export models=~/turnkeyml/models\n",
    "```\n",
    "\n",
    "The `turnkeyml` package only requires the packages to run the tools. If you want to run the models as well, you will also have to install the models' requirements.\n",
    "\n",
    "In your `miniconda` environment:\n",
    "\n",
    "```\n",
    "pip install -r models/requirements.txt\n",
    "```\n",
    "\n",
    "## (Optional) Installing Slurm support\n",
    "\n",
    "Slurm is an open source workload manager for clusters. If you would like to use Slurm to build multiple models simultaneously, please follow the instructions below.\n",
    "\n",
    "### Setup your Slurm environment\n",
    "\n",
    "Ensure that your turnkeyml clone and your conda installation are both inside a shared volume that can be accessed by Slurm.\n",
    "Then, run the following command and wait for the Slurm job to finish:\n",
    "\n",
    "```\n",
    "sbatch --mem=128000 src/turnkeyml/cli/setup_venv.sh\n",
    "```\n",
    "\n",
    "### Get an API token from Huggingface.co (optional)\n",
    "\n",
    "Some models from Huggingface.co might require the use of an API token. You can find your api token under Settings from your Hugging Face account.\n",
    "\n",
    "To allow slurm to use your api token, simply export your token as an environment variable as shown below:\n",
    "\n",
    "\n",
    "```\n",
    "export HUGGINGFACE_API_KEY=<YOUR_API_KEY>\n",
    "```\n",
    "\n",
    "### Setup a shared download folder (optional)\n",
    "\n",
    "Both Torch Hub and Hugging Face models save model content to a local cache. A good practice is to store that data with users that might use the same models using a shared folder. TurnkeyML allows you to setup a shared ML download cache folder when using Slurm by exporting an environment variable as shown below:\n",
    "\n",
    "\n",
    "```\n",
    "export SLURM_ML_CACHE=<PATH_TO_A_SHARED_FOLDER>\n",
    "```\n",
    "\n",
    "### Test it\n",
    "\n",
    "Go to the `models/` folder and build multiple models simultaneously using Slurm.\n",
    "\n",
    "```\n",
    "turnkey selftest/*.py --use-slurm --build-only --cache-dir PATH_TO_A_CACHE_DIR\n",
    "```\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: how do i install dependencies?\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kalin\\miniconda3\\envs\\gaiavenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaiavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
