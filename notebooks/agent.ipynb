{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sourced from [ReactAgent.ipynb](https://colab.research.google.com/drive/1XYNaGvEdyKVbs4g_Maffyq08DUArcW8H?usp=sharing#scrollTo=xEY6G0JtXuMo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "assert openai.api_key, 'Please set OPENAI_API_KEY!'\n",
    "\n",
    "github_token = os.getenv('GITHUB_TOKEN')\n",
    "assert github_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some simple tools to test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
    "    return a - b\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, subtract_tool], llm=llm, verbose=True)\n",
    "\n",
    "response = agent.chat(\"What is 20+(2*4)? Calculate step by step.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input()\n",
    "    if not user_input:\n",
    "        break\n",
    "    print(f\"User: {user_input}\")\n",
    "    response = agent.chat(user_input)\n",
    "    print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = agent.get_prompts()\n",
    "for k, v in prompt_dict.items():\n",
    "    print(f\"Prompt: {k}\\n\\nValue: {v.template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vector index store for Uber and Lyft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "lyft_docs = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/lyft_2021.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "uber_docs = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/uber_2021.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "brainwave_docs = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/ISCA18-Brainwave-CameraReady.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# build index\n",
    "lyft_index = VectorStoreIndex.from_documents(lyft_docs, show_progress=True)\n",
    "uber_index = VectorStoreIndex.from_documents(uber_docs,  show_progress=True)\n",
    "brainwave_index = VectorStoreIndex.from_documents(brainwave_docs, show_progress=True)\n",
    "\n",
    "# persist index\n",
    "lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n",
    "uber_index.storage_context.persist(persist_dir=\"./storage/uber\")\n",
    "brainwave_index.storage_context.persist(persist_dir=\"./storage/brainwave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the query engines w/ top_k set to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n",
    "uber_engine = uber_index.as_query_engine(similarity_top_k=3)\n",
    "brainwave_engine = brainwave_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the query engine tools, each tied to the query engine / vector index with descriptions of each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Lyft financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Uber financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    QueryEngineTool(\n",
    "        query_engine=brainwave_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"ISCA18\",\n",
    "            description=(\n",
    "                \"Provides information about the Brainwave NPU hardware accelerator. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the LLM and agent to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"Compare the revenue growth of Uber and Lyft in 2021.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"What is so special about Brainwave?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input()\n",
    "    if not user_input:\n",
    "        break\n",
    "    print(f\"User: {user_input}\")\n",
    "    response = agent.chat(user_input)\n",
    "    print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Repo Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-readers-github --upgrade\n",
    "!pip install nest_asyncio httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the DiscordReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.github import GithubRepositoryReader\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "\n",
    "github_client = GithubClient(github_token=os.environ[\"GITHUB_TOKEN\"], verbose=True)\n",
    "\n",
    "reader1 = GithubRepositoryReader(\n",
    "    github_client=github_client,\n",
    "    owner=\"onnx\",\n",
    "    repo=\"turnkeyml\",\n",
    "    use_parser=False,\n",
    "    verbose=True,\n",
    "    filter_directories=(\n",
    "        [\"docs\"],\n",
    "        GithubRepositoryReader.FilterType.INCLUDE,\n",
    "    ),\n",
    "    filter_file_extensions=(\n",
    "        [\n",
    "            \".png\",\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".gif\",\n",
    "            \".svg\",\n",
    "            \".ico\",\n",
    "            \"json\",\n",
    "            \".ipynb\",\n",
    "        ],\n",
    "        GithubRepositoryReader.FilterType.EXCLUDE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "repo_docs = reader1.load_data(branch=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# build index\n",
    "repo_index = VectorStoreIndex.from_documents(repo_docs, show_progress=True)\n",
    "\n",
    "# persist index\n",
    "repo_index.storage_context.persist(persist_dir=\"./storage/repo\")\n",
    "repo_engine = repo_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repo_engine.query(\"how do i install turnkeyml repo?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repo_engine.query(\"how do i install dependencies?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=repo_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"repo\",\n",
    "            description=(\n",
    "                \"Provides information about code repository. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "openai_agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"(do not answer implictly, instead use the readme tool)\"\n",
    "queries = [\n",
    "    \"give me the link to Turnkey tests\",\n",
    "    \"give me the link to Turnkey instructions\",\n",
    "    \"what is TurnkeyML's mission?\",\n",
    "    \"how do I get started?\",\n",
    "    \"where is the installation guide?\",\n",
    "    \"how about tutorials?\",\n",
    "    \"what use-cases are there?\",\n",
    "    \"execute the demo.\",\n",
    "    \"what was it benchmarked on?\",\n",
    "    \"generated a report, collect the statistics and summarize in a spreadsheet\",\n",
    "]\n",
    "queries = [\n",
    "    \"how do i install dependencies?\",\n",
    "    \"generte the commands to install dependencies\"\n",
    "]\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    response = openai_agent.chat(f\"{query}\\n{hint}\")\n",
    "    print('-------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
