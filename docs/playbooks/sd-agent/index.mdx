---
title: "Building a Multi-Modal Image Generation Agent"
description: "Learn to build agents that combine LLM reasoning, Stable Diffusion, and Vision models"
icon: "image"
---

<Info>
  **Source Code:** [`src/gaia/agents/sd/agent.py`](https://github.com/amd/gaia/blob/main/src/gaia/agents/sd/agent.py)
</Info>

<Badge text="development" color="orange" />

<video
  controls
  autoPlay
  loop
  muted
  playsInline
  className="w-full rounded-lg"
  src="https://assets.amd-gaia.ai/videos/gaia-sdxl-agent.webm"
/>

## Introduction

This playbook demonstrates building a multi-modal agent by composing image generation and vision capabilities into a single agent class. You'll use Python's multiple inheritance to combine `SDToolsMixin` (Stable Diffusion image generation) and `VLMToolsMixin` (Vision Language Model analysis) with GAIA's base `Agent` class.

The architecture is straightforward: your agent runs an LLM as its reasoning engine, which has access to tools from both mixins. When a user asks to generate an image and create a story, the LLM decides the sequence—call `generate_image()`, then call `create_story_from_image()` to analyze the result. Behind the scenes, [Lemonade Server](https://lemonade-server.ai) hosts the model endpoints locally, providing LLM inference for the agent's reasoning, VLM inference for image analysis, and Stable Diffusion for image generation. Everything runs on your machine with no external API calls.

The key pattern you'll learn is **tool composition through mixins**. Each mixin registers its tools via the `@tool` decorator during initialization. You'll see how domain-specific tools can wrap generic capabilities, creating specialized workflows while keeping the underlying components reusable.

By the end, you'll have a working multi-modal agent and understand how to compose tool capabilities using GAIA's mixin architecture.

---

## Learning Path

<CardGroup cols={3}>
  <Card title="Part 1: Build Your Agent" icon="rocket" href="/playbooks/sd-agent/part-1-building-agent">
    **25 minutes**

    Build and run your first multi-modal agent from scratch.
  </Card>

  <Card title="Part 2: Architecture" icon="diagram-project" href="/playbooks/sd-agent/part-2-architecture">
    **20 minutes**

    Understand how multi-modal agents work under the hood.
  </Card>

  <Card title="Part 3: Variations" icon="wand-magic-sparkles" href="/playbooks/sd-agent/part-3-variations">
    **20 minutes**

    Apply patterns to real-world use cases.
  </Card>
</CardGroup>

---

## Quick Test

<Note>
**First time?** See [Setup Guide](/setup) to install prerequisites (uv, Python, AMD drivers).
</Note>

Want to try it first before building?

Install models and start Lemonade Server:
```bash
gaia init --profile sd
```

Generate one image + story with the built-in agent:
```bash
gaia sd "generate an image of a robot exploring ancient ruins and tell me the story of what it discovers"
```

---

<Card title="Start Building →" icon="play" href="/playbooks/sd-agent/part-1-building-agent" iconType="solid">
  Jump into Part 1 and build your first multi-modal agent
</Card>
