---
title: "Part 1: Build Your First Multi-Modal Agent"
description: "Build and run an agent that generates images and creates stories in 25 minutes"
icon: "rocket"
---

<Info>
  **Time to complete:** 25 minutes
  **Part 1 of 3** - [Overview](/playbooks/sd-agent) ‚Ä¢ Part 2 ‚Üí ‚Ä¢ Part 3 ‚Üí
</Info>

## What You'll Build

By the end of this part, you'll have a fully functional `ImageStoryAgent` class that demonstrates multi-modal AI composition.

**Technically, you're building:**

An agent class that inherits from three parent classes using Python's multiple inheritance:
```python
class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    pass  # ~62 lines of code total including CLI loop
```

**Inheritance chain (MRO):**
```
ImageStoryAgent ‚Üí Agent ‚Üí SDToolsMixin ‚Üí VLMToolsMixin ‚Üí object
```

When you call `super().__init__()`, Python walks this chain to initialize each parent cooperatively.

**What "multi-modal" means here:**

Your agent orchestrates three different AI models through a unified interface:

| Model | Role | Endpoint | Hardware | Context |
|-------|------|----------|----------|---------|
| **Qwen3-8B-GGUF** | Reasoning engine | `POST /v1/chat/completions` | iGPU (Radeon) | 8K tokens |
| **SDXL-Turbo** | Image generation | `POST /images/generations` | CPU | N/A |
| **Qwen3-VL-4B-Instruct-GGUF** | Vision analysis | `POST /v1/chat/completions` (with images) | iGPU (Radeon) | 8K tokens |

**How it works:**
1. Your agent's `process_query()` method sends user input to the **LLM**
2. LLM returns a plan: `[{tool: "generate_image", tool_args: {...}}, {tool: "create_story_from_image", tool_args: {...}}]`
3. Agent executes each tool in sequence:
   - `generate_image()` ‚Üí HTTP POST to Lemonade `/images/generations` ‚Üí Returns image bytes ‚Üí Saved to disk
   - `create_story_from_image()` ‚Üí HTTP POST to Lemonade `/v1/chat/completions` with image ‚Üí Returns story text
4. LLM synthesizes final answer combining all results

**Model formats and acceleration:**
- GGUF models (Qwen3-8B, Qwen3-VL-4B) ‚Üí Run on **iGPU (Radeon)** via Vulkan
- Safetensors model (SDXL-Turbo) ‚Üí Currently runs on **CPU** (GPU acceleration planned)

Total memory: ~15GB loaded.

**What you'll be able to do:**

```python
agent = ImageStoryAgent()
result = agent.process_query("create a cyberpunk street scene with a story")

# Agent automatically:
# 1. Calls generate_image() ‚Üí creates image in ~17s
# 2. Calls create_story_from_image() ‚Üí generates 2-3 paragraph narrative in ~15s
# 3. Returns both image path and story text in final answer
```

**The 6 tools your agent will have:**

From `SDToolsMixin` (registered in `init_sd()`):

- **`generate_image(prompt: str, model: Optional[str], size: Optional[str], steps: Optional[int], cfg_scale: Optional[float], seed: Optional[int]) -> Dict[str, Any]`**
  - Calls: `LemonadeClient.generate_image()` ‚Üí `POST /images/generations`
  - Returns: `{"status": "success", "image_path": "...", "model": "SDXL-Turbo", "generation_time_s": 17.2, "seed": 123456}`
  - Saves: PNG file to `output_dir`, adds to `self.sd_generations` list

- **`list_sd_models() -> Dict[str, Any]`**
  - Returns: Available models with speed/quality characteristics

- **`get_generation_history(limit: int = 10) -> Dict[str, Any]`**
  - Returns: Recent generations from `self.sd_generations` session state

From `VLMToolsMixin` (registered in `init_vlm()`):

- **`analyze_image(image_path: str, focus: str = "all") -> Dict[str, Any]`**
  - Calls: `VLMClient.analyze()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "description": "...", "focus_analysis": "..."}`

- **`create_story_from_image(image_path: str, story_style: str = "any") -> Dict[str, Any]`**
  - Calls: `VLMClient.create_story()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "story": "2-3 paragraph narrative", "description": "..."}`

- **`answer_question_about_image(image_path: str, question: str) -> Dict[str, Any]`**
  - Calls: `VLMClient.answer_question()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "answer": "...", "confidence": "high"}`

**How tools get registered (under the hood):**

When you call `self.init_sd()` and `self.init_vlm()`:

1. **`init_sd()` creates closure functions** with `@tool` decorator:
   ```python
   @tool(atomic=True, name="generate_image", description="...", parameters={...})
   def generate_image(prompt, model=None, ...):
       return self._generate_image(prompt, model, ...)  # Calls instance method
   ```

2. **The `@tool` decorator** (defined in `gaia.agents.base.tools`):
   - Wraps the function with metadata
   - Adds it to global `_TOOL_REGISTRY` dict: `{tool_name: {function, schema, ...}}`
   - Returns the wrapped function

3. **At runtime**, when LLM calls a tool:
   - Agent looks up tool in `_TOOL_REGISTRY` by name
   - Calls the function with LLM-provided arguments
   - Returns result to LLM for next reasoning step

**Instance state created:**

After initialization, your agent instance has:
```python
self.sd_client          # LemonadeClient(base_url="http://localhost:8000")
self.sd_output_dir      # Path("./generated_images")
self.sd_default_model   # "SDXL-Turbo"
self.sd_generations     # [] - List of dicts tracking generated images
self.vlm_client         # VLMClient(vlm_model="Qwen3-VL-4B-Instruct-GGUF")
self.vlm_model          # "Qwen3-VL-4B-Instruct-GGUF"
```

Plus all the `Agent` base class state (tool registry, conversation history, LLM client).

---

## Prerequisites

<Steps>
  <Step title="Complete setup">
    If you haven't already, follow the [Setup guide](/setup) to install prerequisites (uv, Lemonade Server, etc.).
  </Step>

  <Step title="Create project folder">
    ```bash
    mkdir my-sd-agent
    cd my-sd-agent
    ```
  </Step>

  <Step title="Create virtual environment">
    ```bash
    uv venv .venv --python 3.12
    ```

    <Note>uv will automatically download Python 3.12 if not already installed.</Note>

    Activate the environment:

    <Tabs>
      <Tab title="Linux / macOS">
        ```bash
        source .venv/bin/activate
        ```
      </Tab>
      <Tab title="Windows">
        ```bash
        .venv\Scripts\activate
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step title="Install GAIA">
    ```bash
    uv pip install amd-gaia
    ```
  </Step>

  <Step title="Initialize SD profile">
    ```bash
    gaia init --profile sd
    ```

    This command:
    1. Installs Lemonade Server (if not already installed)
    2. **Starts Lemonade Server automatically**
    3. Downloads three models (~15GB):
       - **SDXL-Turbo** (6.5GB) - Diffusion model in safetensors format
       - **Qwen3-8B-GGUF** (5GB) - Agent LLM in quantized GGUF format
       - **Qwen3-VL-4B-Instruct-GGUF** (3.2GB) - Vision LLM in quantized GGUF format
    4. Verifies the setup is working

    **Hardware acceleration:**
    - GGUF models run on iGPU (Radeon integrated graphics) via Vulkan
    - SDXL-Turbo currently runs on CPU (GPU acceleration planned)

    <video
      controls
      className="w-full rounded-lg"
      src="https://assets.amd-gaia.ai/videos/gaia-init-sd-profile.webm"
    />
    <Note>Video: Running `gaia init --profile sd` and watching models download</Note>
  </Step>
</Steps>

---

## Building the Agent

### Step 1: Class Definition

Create a new file `image_story_agent.py` and start with the class definition:

```python
from gaia.agents.base import Agent
from gaia.sd import SDToolsMixin
from gaia.vlm import VLMToolsMixin

class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    """
    Multi-modal agent that generates images and creates stories.

    Inherits from:
    - Agent: Base class providing reasoning loop, tool registry, LLM interface
    - SDToolsMixin: Adds Stable Diffusion image generation capabilities
    - VLMToolsMixin: Adds vision-language model analysis capabilities
    """
    pass
```

---

### Step 2: Implement `__init__` and System Prompt

Add the `__init__` method to initialize both mixins, plus the required abstract methods:

<Accordion title="Model Selection Reference">
Available SD models (set in `default_model` parameter):

| Model | Speed | Quality | Best For |
|-------|-------|---------|----------|
| **SDXL-Turbo** (default) | ~17s | ‚≠ê‚≠ê‚≠ê | Balanced speed/quality |
| **SD-Turbo** | ~13s | ‚≠ê‚≠ê | Rapid prototyping |
| **SDXL-Base-1.0** | ~9min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Photorealistic images |

Performance measured on AMD Ryzen AI MAX+ 395 with Radeon 8060S
</Accordion>

Now add three methods to the class:

```python
    def __init__(self, output_dir="./generated_images"):
        # Initialize Agent base class with Qwen3-8B model
        # (SD profile downloads this model, not the default Qwen3-Coder-30B)
        super().__init__(
            model_id="Qwen3-8B-GGUF",
            min_context_size=8192,  # 8K context sufficient for image workflows
        )

        # Initialize SD tools
        # Under the hood: Creates LemonadeClient, sets output directory,
        # and registers tools via @tool decorator (generate_image, list_sd_models, etc.)
        self.init_sd(
            output_dir=output_dir,
            default_model="SDXL-Turbo",  # Fast stylized model (4 steps, ~17s)
        )

        # Initialize VLM tools
        # Under the hood: Creates VLMClient for vision analysis,
        # and registers tools (analyze_image, create_story_from_image, etc.)
        self.init_vlm(model="Qwen3-VL-4B-Instruct-GGUF")

    def _register_tools(self):
        # Demonstrate creating custom tools that extend mixin capabilities
        # Pattern: Wrap generic mixin tools with specialized behavior
        from gaia.agents.base.tools import tool

        @tool(name="create_story_about_image")
        def create_story_about_image(image_path: str) -> dict:
            """
            Create an adventure story about an image with metadata tracking.

            Wraps VLMToolsMixin's generic create_story_from_image() to add:
            - Fixed adventure style (specialized vs. parameterized)
            - Custom metadata for tracking agent/style
            - Could extend with: DB saving, markdown formatting, etc.
            """
            # Call VLM mixin's internal method (available via inheritance)
            result = self._create_story_from_image(image_path, story_style="adventure")

            # Extend with custom metadata
            if result.get("status") == "success":
                result["custom_metadata"] = {
                    "agent": "ImageStoryAgent",
                    "style": "adventure",
                }

            return result

    def _get_system_prompt(self) -> str:
        # Add custom instructions (mixin prompts are auto-composed)
        # Only explain what's NEW/CUSTOM, not what mixins already provide
        return """
When creating stories about images, prefer the custom create_story_about_image tool
which automatically uses adventure style and tracks metadata."""
```

**What's happening with system prompts:**

The Agent base class automatically composes the final system prompt:
1. Calls `_get_mixin_prompts()` ‚Üí Gets `[sd_prompt, vlm_prompt]` from mixins (automatic)
2. Calls `_get_system_prompt()` ‚Üí Gets your custom instructions about the custom tool
3. Composes final prompt: `sd_prompt + "\n\n" + vlm_prompt + "\n\n" + custom`

The LLM receives:
- SD prompt engineering guidelines (from SDToolsMixin)
- VLM usage guidelines (from VLMToolsMixin)
- Your custom tool instructions (from _get_system_prompt)
- Tool schemas for all 7 tools (6 from mixins + 1 custom)

Simpler option - use only mixin prompts:
```python
def _get_system_prompt(self) -> str:
    return ""  # No custom additions, just use mixin prompts
```

---

### Step 3: Add CLI Loop and Run

Complete the agent by adding a CLI loop. Here's the full `image_story_agent.py` file:

```python
from gaia.agents.base import Agent
from gaia.sd import SDToolsMixin
from gaia.vlm import VLMToolsMixin

class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    """Multi-modal agent that generates images and creates stories."""

    def __init__(self, output_dir="./generated_images"):
        # Initialize Agent base class with Qwen3-8B model
        # (SD profile downloads this model, not the default Qwen3-Coder-30B)
        super().__init__(
            model_id="Qwen3-8B-GGUF",
            min_context_size=8192,  # 8K context sufficient for image workflows
        )

        # Step 2: Initialize SD tools
        # Under the hood: Creates LemonadeClient, sets output directory,
        # and registers tools via @tool decorator (generate_image, list_sd_models, etc.)
        self.init_sd(
            output_dir=output_dir,
            default_model="SDXL-Turbo",  # Fast stylized model (4 steps, ~17s)
        )

        # Step 3: Initialize VLM tools
        # Under the hood: Creates VLMClient for vision analysis,
        # and registers tools (analyze_image, create_story_from_image, etc.)
        self.init_vlm(model="Qwen3-VL-4B-Instruct-GGUF")

    def _register_tools(self):
        # Demonstrate creating custom tools that extend mixin capabilities
        # Pattern: Wrap generic mixin tools with specialized behavior
        from gaia.agents.base.tools import tool

        @tool(name="create_story_about_image")
        def create_story_about_image(image_path: str) -> dict:
            """
            Create an adventure story about an image with metadata tracking.

            Wraps VLMToolsMixin's generic create_story_from_image() to add:
            - Fixed adventure style (specialized vs. parameterized)
            - Custom metadata for tracking agent/style
            - Could extend with: DB saving, markdown formatting, etc.
            """
            # Call VLM mixin's internal method (available via inheritance)
            result = self._create_story_from_image(image_path, story_style="adventure")

            # Extend with custom metadata
            if result.get("status") == "success":
                result["custom_metadata"] = {
                    "agent": "ImageStoryAgent",
                    "style": "adventure",
                }

            return result

    def _get_system_prompt(self) -> str:
        # Add custom instructions (mixin prompts are auto-composed)
        # Only explain what's NEW/CUSTOM, not what mixins already provide
        return """
When creating stories about images, prefer the custom create_story_about_image tool
which automatically uses adventure style and tracks metadata."""


if __name__ == "__main__":
    import os

    # Create output directory for generated images
    os.makedirs("./generated_images", exist_ok=True)

    # Initialize the agent (this triggers all the __init__ steps above)
    agent = ImageStoryAgent()

    print("Image Story Agent ready! Type 'quit' to exit.\n")

    # Interactive CLI loop
    while True:
        user_input = input("You: ").strip()

        if user_input.lower() in ("quit", "exit", "q"):
            print("Goodbye!")
            break

        if not user_input:
            continue

        # Process the query through the agent's reasoning loop
        # Under the hood: LLM plans which tools to call, executes them, synthesizes response
        result = agent.process_query(user_input)

        # Display the agent's final answer
        if result.get("final_answer"):
            print(f"\nAgent: {result['final_answer']}\n")
        else:
            print("\nAgent: (no response)\n")
```

Run it:
```bash
python image_story_agent.py
```

Try these prompts:
```
You: generate an image of a robot exploring ancient ruins and create a story about it
You: create a sunset over mountains and tell me an adventure story
You: make a cyberpunk street scene and describe what you see
```

The agent will:
- Use `generate_image()` for image creation (from SDToolsMixin)
- Use `analyze_image()` for descriptions (from VLMToolsMixin)
- Use `create_story_about_image()` for stories (your custom tool)

---

## Understanding Custom Tools

The example above demonstrates an important pattern: **creating custom tools that extend mixin capabilities**.

### Why Create Custom Tools?

**Mixin tools are generic** - they work for any use case:
- `create_story_from_image(image_path, story_style)` - Flexible, parameterized

**Custom tools are specialized** - tailored to your specific needs:
- `create_story_about_image(image_path)` - Fixed style, custom metadata, specialized behavior

### The Wrapper Pattern

```python
@tool(name="create_story_about_image")
def create_story_about_image(image_path: str) -> dict:
    # 1. Call mixin's internal method (available via inheritance)
    result = self._create_story_from_image(image_path, story_style="adventure")

    # 2. Extend the result with custom behavior
    if result.get("status") == "success":
        result["custom_metadata"] = {
            "agent": "ImageStoryAgent",
            "style": "adventure",
        }

    return result
```

**What this demonstrates:**
1. **Access internal methods** - `self._create_story_from_image()` is a private method from VLMToolsMixin, accessible via inheritance
2. **Wrap, don't replace** - Use mixin functionality, add custom behavior on top
3. **Specialized interface** - Simpler for your use case (no style parameter needed)
4. **Extensible** - Could add: DB saving, markdown formatting, logging, etc.

### When to Create Custom Tools

**Use mixin tools directly when:**
- Generic functionality is sufficient
- You need parameterization (different styles, options)
- No custom pre/post processing needed

**Create custom tools when:**
- You want fixed, specialized behavior (always adventure style)
- You need to track metadata or logging
- You want to compose multiple operations (analyze + story + save to DB)
- You're building a domain-specific interface

### Real-World Example: Asset Management

```python
@tool(name="create_and_catalog_image")
def create_and_catalog_image(prompt: str, tags: list[str]) -> dict:
    """Generate image and save metadata to database."""

    # 1. Use SD mixin to generate
    img_result = self._generate_image(prompt)

    if img_result.get("status") == "success":
        # 2. Use VLM mixin to analyze
        analysis = self._analyze_image(img_result["image_path"])

        # 3. Custom: Save to database
        self.execute_sql(
            "INSERT INTO images (path, prompt, tags, description) VALUES (?, ?, ?, ?)",
            (img_result["image_path"], prompt, ",".join(tags), analysis["description"])
        )

        img_result["cataloged"] = True

    return img_result
```

This pattern shows the power of composition: mixin tools + custom orchestration.

---

## Example Output

Here's what you'll see when you run the agent:

```
You: create a robot exploring ancient ruins

Agent: I'll generate that image for you.

[Generating image with SDXL-Turbo (4 steps, 512x512)...]
‚úì Image saved to ./generated_images/robot_exploring_ancient_ruins_SDXL-Turbo_20250202_143022.png

[Analyzing image and creating story...]

Here's a story about your image:

In the heart of a forgotten civilization, a small reconnaissance robot named
Unit-7 carefully navigates through towering stone pillars covered in mysterious
glyphs. Its optical sensors scan the intricate carvings, attempting to decode
the ancient language that has been silent for millennia.

The robot's metallic frame gleams in the filtered sunlight that breaks through
the overgrown canopy above. Each step is calculated, avoiding the treacherous
gaps between fallen columns and the thick vines that have claimed this place
as their own.

As Unit-7's processors work to piece together the historical puzzle, it transmits
its findings back to the research station, unaware that it may be the first
sentient being to witness these ruins in over a thousand years.
```

**Performance (AMD Ryzen AI MAX+ 395):**
- Image generation: ~17s (SDXL-Turbo, 512x512)
- VLM story creation: ~15-20s (Qwen3-VL-4B)
- **Total time: ~35-40s** for image + story

<video
  controls
  className="w-full rounded-lg"
  src="https://assets.amd-gaia.ai/videos/image-story-agent-demo.webm"
/>
<Note>Video: Running the ImageStoryAgent and generating image + story</Note>

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Connection Error: Cannot connect to Lemonade Server">
    **Error:** `LemonadeClientError: Cannot connect to http://localhost:8000`

    **Solution:** Lemonade Server isn't running. Check status:
    ```bash
    lemonade-server status
    ```

    If not running, `gaia init --profile sd` should have started it. Try restarting:
    ```bash
    lemonade-server serve
    ```
  </Accordion>

  <Accordion title="Model not found error">
    **Error:** `Model 'SDXL-Turbo' not found`

    **Solution:** Download missing models:
    ```bash
    lemonade-server pull SDXL-Turbo
    lemonade-server pull Qwen3-VL-4B-Instruct-GGUF
    ```

    Or re-run init:
    ```bash
    gaia init --profile sd
    ```
  </Accordion>

  <Accordion title="Import error: No module named 'gaia.sd'">
    **Error:** `ModuleNotFoundError: No module named 'gaia.sd'`

    **Solution:** Upgrade GAIA:
    ```bash
    uv pip install --upgrade amd-gaia
    ```

    Verify installation:
    ```bash
    python -c "from gaia.sd import SDToolsMixin; print('OK')"
    ```
  </Accordion>

  <Accordion title="How to get reproducible results">
    **Question:** How do I generate the same image twice?

    **Solution:** Use a fixed seed:

    CLI:
    ```bash
    gaia sd "robot kitten" --seed 42
    ```

    Python:
    ```python
    result = agent.process_query("generate robot kitten with seed 42")
    ```

    By default, images use random seeds for variety. Specify a seed for reproducibility.
  </Accordion>
</AccordionGroup>

---

## What's Next?

üéâ **Congratulations!** You've built your first multi-modal agent. It works, but *how* does it work?

<Card title="Part 2: Architecture Deep Dive ‚Üí" icon="diagram-project" href="/playbooks/sd-agent/part-2-architecture">
  Now that you've built it, understand how multi-modal agents work under the hood. Learn about the reasoning loop, tool composition patterns, and mixin system.
</Card>
