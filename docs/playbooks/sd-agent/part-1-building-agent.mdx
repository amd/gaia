---
title: "Part 1: Build Your First Multi-Modal Agent"
description: "Build and run an agent that generates images and creates stories in 25 minutes"
icon: "rocket"
---

<Info>
  **Time to complete:** 25 minutes
  **Part 1 of 3** - [Overview](/playbooks/sd-agent) â€¢ Part 2 â†’ â€¢ Part 3 â†’
</Info>

## What You'll Build

An `ImageStoryAgent` that generates images and creates stories about them. ~60 lines of code total.

When you run `agent.process_query("create a robot exploring ancient ruins")`, here's what happens:

1. **LLM decides the plan**: Qwen3-8B analyzes your query and returns `{plan: [{tool: "generate_image", tool_args: {prompt: "..."}}, {tool: "create_story_from_image", tool_args: {...}}]}`
2. **Executes generate_image()**: Sends prompt to SDXL-Turbo (4-step diffusion model), receives PNG bytes, saves to disk (~17s)
3. **Executes create_story_from_image()**: Calls your custom tool â†’ sends image bytes to Qwen3-VL-4B vision model â†’ receives 2-3 paragraph narrative (~15s)
4. **Returns result**: `{"status": "success", "result": "Here's your image and story...", "conversation": [...], "steps_taken": 3}`

You'll create the `create_story_from_image` tool yourself using VLM capabilities.

**Three models, one agent:**
- **Qwen3-8B-GGUF** (5GB): Reasoning engine - decides which tools to call and when
- **SDXL-Turbo** (6.5GB): Diffusion model - generates images from text prompts
- **Qwen3-VL-4B** (3.2GB): Vision-language model - analyzes images and generates text

All run locally via Lemonade Server. GGUF models use AMD Radeon iGPU (Vulkan), SDXL runs on CPU.

---

## Prerequisites

<Steps>
  <Step title="Complete setup">
    If you haven't already, follow the [Setup guide](/setup) to install prerequisites (uv, Lemonade Server, etc.).
  </Step>

  <Step title="Create project folder">
    ```bash
    mkdir my-sd-agent
    cd my-sd-agent
    ```
  </Step>

  <Step title="Create virtual environment">
    ```bash
    uv venv .venv --python 3.12
    ```

    <Note>uv will automatically download Python 3.12 if not already installed.</Note>

    Activate the environment:

    <Tabs>
      <Tab title="Linux / macOS">
        ```bash
        source .venv/bin/activate
        ```
      </Tab>
      <Tab title="Windows">
        ```bash
        .venv\Scripts\activate
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step title="Install GAIA">
    ```bash
    uv pip install amd-gaia
    ```
  </Step>

  <Step title="Initialize SD profile">
    ```bash
    gaia init --profile sd
    ```

    This command:
    1. Installs Lemonade Server (if not already installed)
    2. **Starts Lemonade Server automatically**
    3. Downloads three models (~15GB):
       - **SDXL-Turbo** (6.5GB) - Diffusion model in safetensors format
       - **Qwen3-8B-GGUF** (5GB) - Agent LLM in quantized GGUF format
       - **Qwen3-VL-4B-Instruct-GGUF** (3.2GB) - Vision LLM in quantized GGUF format
    4. Verifies the setup is working

    **Hardware acceleration:**
    - GGUF models run on iGPU (Radeon integrated graphics) via Vulkan
    - SDXL-Turbo currently runs on CPU (GPU acceleration planned)

    <video
      controls
      className="w-full rounded-lg"
      src="https://assets.amd-gaia.ai/videos/gaia-init-sd-profile.webm"
    />
    <Note>Video: Running `gaia init --profile sd` and watching models download</Note>
  </Step>
</Steps>

---

## Building the Agent

Create a new file `image_story_agent.py` and add this code:

### Step 1: Initialize the Agent

```python
from gaia.agents.base import Agent
from gaia.sd import SDToolsMixin
from gaia.vlm import VLMToolsMixin

class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    """Agent that generates images and creates stories."""

    def __init__(self, output_dir="./generated_images"):
        super().__init__(model_id="Qwen3-8B-GGUF")
        self.init_sd(output_dir=output_dir, default_model="SDXL-Turbo")
        self.init_vlm(model="Qwen3-VL-4B-Instruct-GGUF")
```

**What this does:**
- `Agent, SDToolsMixin, VLMToolsMixin`: Inherits three capabilities - reasoning, image generation, vision analysis
- `super().__init__(model_id="Qwen3-8B-GGUF")`: Sets up the reasoning LLM (downloaded by `gaia init --profile sd`)
- `init_sd()`: Registers image generation tools (`generate_image`, `list_sd_models`, `get_generation_history`)
- `init_vlm()`: Registers vision tools (`analyze_image`, `answer_question_about_image`)

The agent now has 5 tools from mixins. You'll add a 6th custom tool next.

<AccordionGroup>
  <Accordion title="View all 5 mixin tools and their signatures">
    **From SDToolsMixin:**

    ```python
    generate_image(
        prompt: str,
        model: str = None,
        size: str = None,
        steps: int = None,
        cfg_scale: float = None,
        seed: int = None
    ) -> dict
    ```
    - Sends prompt to SDXL-Turbo diffusion model
    - Returns: `{"status": "success", "image_path": "...", "generation_time_s": 17.2, "seed": 123456}`
    - Saves PNG to `output_dir`

    ```python
    list_sd_models() -> dict
    ```
    - Returns available models with speed/quality characteristics

    ```python
    get_generation_history(limit: int = 10) -> dict
    ```
    - Returns recent generations from this session

    **From VLMToolsMixin:**

    ```python
    analyze_image(image_path: str, focus: str = "all") -> dict
    ```
    - Sends image bytes to Qwen3-VL-4B vision model
    - Returns: `{"status": "success", "description": "...", "focus_analysis": "..."}`

    ```python
    answer_question_about_image(image_path: str, question: str) -> dict
    ```
    - Sends image + question to VLM
    - Returns: `{"status": "success", "answer": "...", "confidence": "high"}`
  </Accordion>

  <Accordion title="Available SD models">
    You can change `default_model` in `init_sd()`:

    | Model | Speed | Quality |
    |-------|-------|---------|
    | **SDXL-Turbo** (default) | ~17s | Balanced |
    | **SD-Turbo** | ~13s | Fast prototyping |
    | **SDXL-Base-1.0** | ~9min | Photorealistic |
  </Accordion>
</AccordionGroup>

---

### Step 2: Add Custom Tool

Add `_register_tools()` to create `create_story_from_image`:

```python
    def _register_tools(self):
        from gaia.agents.base.tools import tool
        from pathlib import Path

        @tool(atomic=True)
        def create_story_from_image(image_path: str, story_style: str = "any") -> dict:
            """Generate a creative short story (2-3 paragraphs) based on an image."""
            path = Path(image_path)
            if not path.exists():
                return {"status": "error", "error": f"Image not found: {image_path}"}

            # Read image bytes
            image_bytes = path.read_bytes()

            # Build story prompt based on style
            style_map = {
                "whimsical": "playful and lighthearted",
                "dramatic": "intense and emotionally charged",
                "adventure": "exciting with action and discovery",
                "any": "engaging and imaginative"
            }
            style_desc = style_map.get(story_style, "engaging and imaginative")

            # Call VLM to generate story
            prompt = f"Create a short creative story (2-3 paragraphs) that is {style_desc}. Bring the image to life with narrative."
            story = self.vlm_client.extract_from_image(image_bytes, prompt=prompt)

            return {
                "status": "success",
                "story": story,
                "story_style": story_style,
                "image_path": str(path)
            }
```

**What this does:**
- `@tool(atomic=True)`: Registers `create_story_from_image` as an atomic tool
  - **atomic=True**: Tool is self-contained - executes completely without multi-step planning
  - Most tools are atomic (generate image, analyze image, query database)
  - Non-atomic: multi-step tasks like "research a topic"
- Reads image bytes from file path
- Builds a storytelling prompt based on the style parameter
- Calls `self.vlm_client.extract_from_image()` - sends image + prompt to Qwen3-VL-4B vision model
- Returns structured dict with story text and metadata
- Decorator auto-generates JSON schema from function signature and docstring

---

### Step 3: Add System Prompt (Optional)

Add `_get_system_prompt()` to customize instructions:

```python
    def _get_system_prompt(self) -> str:
        return """You are an image generation and storytelling agent.

WORKFLOW for image + story requests:
1. Call generate_image() - THIS RETURNS: {"status": "success", "image_path": ".gaia/cache/sd/images/..."}
2. Extract the ACTUAL image_path value from step 1's result
3. Call create_story_from_image(image_path=<ACTUAL PATH FROM STEP 1>, story_style=<user's tone>)
4. Include the COMPLETE story text in your final answer

CRITICAL: Extract actual values from tool results, NEVER use placeholders like "$IMAGE_PATH$" or "generated_image_path"

CORRECT parameter passing:
Step 1 result: {"image_path": ".gaia/cache/sd/images/robot_123.png"}
Step 2 args: {"image_path": ".gaia/cache/sd/images/robot_123.png", "story_style": "adventure"} âœ“

INCORRECT (DO NOT DO THIS):
Step 2 args: {"image_path": "$IMAGE_PATH$"} âœ—
Step 2 args: {"image_path": "generated_image_path"} âœ—

OTHER RULES:
- Generate ONE image by default (multiple only if explicitly requested: "3 images", "variations")
- Match story_style to user's request: "whimsical" (cute/playful), "adventure" (action), "dramatic" (intense), "any" (default)
- Include full story text in answer - users want to read it immediately"""
```

**What this does:**
- Documents your custom `create_story_from_image` tool (parameters, return value, purpose)
- Provides explicit workflow: generate image â†’ create story using the image_path â†’ include full text in answer
- Sets critical rules: use image_path from results, include complete story text, default to one image
- Gives example showing proper parameter passing between tool calls

**Why this is important:**
- Mixins only describe their own tools (SD guidelines don't know about VLM or custom tools)
- Your custom tool needs explicit documentation and usage instructions
- Clear workflow prevents errors like missing image_path or incomplete responses
- Examples help the LLM understand expected behavior

<Note>
Tool schemas (parameters, types) are auto-added by the `@tool` decorator. Your system prompt provides usage guidelines and workflow that schemas can't express.
</Note>

---

### Step 4: Add CLI and Run

Add the CLI loop to test your agent:

```python
if __name__ == "__main__":
    import os

    os.makedirs("./generated_images", exist_ok=True)
    agent = ImageStoryAgent()

    print("Image Story Agent ready! Type 'quit' to exit.\n")

    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in ("quit", "exit", "q"):
            break
        if user_input:
            result = agent.process_query(user_input)
            if result.get("result"):
                print(f"\nAgent: {result['result']}\n")
```

<Accordion title="How the CLI works">

**Quick summary:**
- Creates output directory and initializes agent (loads Qwen3-8B, registers 6 tools)
- Loops: reads input â†’ calls `process_query(input)` â†’ prints result
- `process_query()` handles LLM planning and tool execution automatically

<Accordion title="See detailed step-by-step execution">
**When you call `agent.process_query("create a robot exploring ruins")`:**

**Step 1 - LLM Planning:**
```python
# Agent sends to Qwen3-8B
{
  "messages": [{"role": "user", "content": "create a robot exploring ruins"}],
  "tools": [
    {"name": "generate_image", "parameters": {"prompt": "str", ...}},
    {"name": "create_story_from_image", "parameters": {"image_path": "str", ...}},
    # ... other tools
  ]
}
```

**Step 2 - LLM Response:**
```python
# LLM decides the plan
{
  "plan": [
    {"tool": "generate_image", "tool_args": {"prompt": "robot exploring ancient ruins"}},
    {"tool": "create_story_from_image", "tool_args": {"image_path": "..."}}
  ]
}
```

**Step 3 - Tool Execution:**
- Calls `generate_image("robot exploring ancient ruins")` â†’ Saves PNG to disk (~17s)
- Calls `create_story_from_image("path/to/image.png")` â†’ Gets story from VLM (~15s)

**Step 4 - Final Synthesis:**
- Agent sends tool results back to LLM
- LLM creates final answer combining both results

**Step 5 - Return Value:**
```python
{
  "status": "success",
  "result": "I created an image of a robot exploring ancient ruins and wrote a story about it...",
  "conversation": [...],  # Full conversation history
  "steps_taken": 3,
  "duration": 42.3
}
```

**Agent limits:** Max 20 steps (configurable). Stops when LLM returns `answer` field instead of requesting more tools.
</Accordion>
</Accordion>

**Run it:**
```bash
python image_story_agent.py
```

<Tip>
Or skip the typing and run the complete example from the GAIA repository:
```bash
python examples/sd_agent_example.py
```
</Tip>

**Try:**
```
You: create a robot exploring ancient ruins
You: generate a sunset over mountains and describe the colors
You: make a cyberpunk street scene and tell me a story
```

---

## Understanding Custom Tools

The example above demonstrates an important pattern: **creating custom tools that extend mixin capabilities**.

### Why Create Custom Tools?

**Mixin tools are generic** - they work for any use case:
- `create_story_from_image(image_path, story_style)` - Flexible, parameterized

**Custom tools are specialized** - tailored to your specific needs:
- `create_story_about_image(image_path)` - Fixed style, custom metadata, specialized behavior

### The Wrapper Pattern

```python
@tool
def create_story_about_image(image_path: str) -> dict:
    """Create a story about an image with metadata tracking."""
    # 1. Call the mixin method
    result = self._create_story_from_image(image_path, story_style="adventure")

    # 2. Add custom behavior
    if result.get("status") == "success":
        result["custom_metadata"] = {
            "agent": "ImageStoryAgent",
            "style": "adventure",
        }

    return result
```

**Pattern:**
1. Call mixin methods (like `self._create_story_from_image()`) - these are the implementation methods mixins use internally
2. Add custom behavior: metadata, logging, database operations, formatting
3. Return the modified result

**Why underscore methods?** Mixins register public tools (`create_story_from_image`) and keep implementation methods private (`_create_story_from_image`). You inherit both - use the underscore methods in custom tools to avoid double-registration.

### When to Create Custom Tools

**Use mixin tools directly when:**
- Generic functionality is sufficient
- You need parameterization (different styles, options)
- No custom pre/post processing needed

**Create custom tools when:**
- You want fixed, specialized behavior (always adventure style)
- You need to track metadata or logging
- You want to compose multiple operations (analyze + story + save to DB)
- You're building a domain-specific interface

### Real-World Example: Asset Management

```python
@tool
def create_and_catalog_image(prompt: str, tags: list[str]) -> dict:
    """Generate image and save metadata to database."""

    # 1. Use SD mixin to generate
    img_result = self._generate_image(prompt)

    if img_result.get("status") == "success":
        # 2. Use VLM mixin to analyze
        analysis = self._analyze_image(img_result["image_path"])

        # 3. Custom: Save to database
        self.execute_sql(
            "INSERT INTO images (path, prompt, tags, description) VALUES (?, ?, ?, ?)",
            (img_result["image_path"], prompt, ",".join(tags), analysis["description"])
        )

        img_result["cataloged"] = True

    return img_result
```

This pattern shows the power of composition: mixin tools + custom orchestration.

---

## Example Output

Here's what you'll see when you run the agent:

```
You: create a robot exploring ancient ruins

Agent: I'll generate that image for you.

[Generating image with SDXL-Turbo (4 steps, 512x512)...]
âœ“ Image saved to ./generated_images/robot_exploring_ancient_ruins_SDXL-Turbo_20250202_143022.png

[Analyzing image and creating story...]

Here's a story about your image:

In the heart of a forgotten civilization, a small reconnaissance robot named
Unit-7 carefully navigates through towering stone pillars covered in mysterious
glyphs. Its optical sensors scan the intricate carvings, attempting to decode
the ancient language that has been silent for millennia.

The robot's metallic frame gleams in the filtered sunlight that breaks through
the overgrown canopy above. Each step is calculated, avoiding the treacherous
gaps between fallen columns and the thick vines that have claimed this place
as their own.

As Unit-7's processors work to piece together the historical puzzle, it transmits
its findings back to the research station, unaware that it may be the first
sentient being to witness these ruins in over a thousand years.
```

**Performance (AMD Ryzen AI MAX+ 395):**
- Image generation: ~17s (SDXL-Turbo, 512x512)
- VLM story creation: ~15-20s (Qwen3-VL-4B)
- **Total time: ~35-40s** for image + story

<video
  controls
  className="w-full rounded-lg"
  src="https://assets.amd-gaia.ai/videos/image-story-agent-demo.webm"
/>
<Note>Video: Running the ImageStoryAgent and generating image + story</Note>

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Connection Error: Cannot connect to Lemonade Server">
    **Error:** `LemonadeClientError: Cannot connect to http://localhost:8000`

    **Solution:** Lemonade Server isn't running. Check status:
    ```bash
    lemonade-server status
    ```

    If not running, `gaia init --profile sd` should have started it. Try restarting:
    ```bash
    lemonade-server serve
    ```
  </Accordion>

  <Accordion title="Model not found error">
    **Error:** `Model 'SDXL-Turbo' not found`

    **Solution:** Download missing models:
    ```bash
    lemonade-server pull SDXL-Turbo
    lemonade-server pull Qwen3-VL-4B-Instruct-GGUF
    ```

    Or re-run init:
    ```bash
    gaia init --profile sd
    ```
  </Accordion>

  <Accordion title="Import error: No module named 'gaia.sd'">
    **Error:** `ModuleNotFoundError: No module named 'gaia.sd'`

    **Solution:** Upgrade GAIA:
    ```bash
    uv pip install --upgrade amd-gaia
    ```

    Verify installation:
    ```bash
    python -c "from gaia.sd import SDToolsMixin; print('OK')"
    ```
  </Accordion>

  <Accordion title="How to get reproducible results">
    **Question:** How do I generate the same image twice?

    **Solution:** Use a fixed seed:

    CLI:
    ```bash
    gaia sd "robot kitten" --seed 42
    ```

    Python:
    ```python
    result = agent.process_query("generate robot kitten with seed 42")
    ```

    By default, images use random seeds for variety. Specify a seed for reproducibility.
  </Accordion>
</AccordionGroup>

---

## Extending Your Agent

### Adding More Capabilities

Mixins add functionality to agents. Available mixins:

```python
from gaia.rag import RAGToolsMixin        # Search documents, answer from PDFs
from gaia.database import DatabaseMixin   # Execute SQL queries
from gaia.audio import AudioToolsMixin    # Speech-to-text, text-to-speech

class MultiCapabilityAgent(Agent, SDToolsMixin, VLMToolsMixin, RAGToolsMixin):
    def __init__(self):
        super().__init__(model_id="Qwen3-8B-GGUF")
        self.init_sd()
        self.init_vlm()
        self.init_rag()
```

Each mixin adds tools the LLM can call. The agent decides which tools to use based on the query.

### Changing Models

Use different models for different needs:

```python
# Faster inference, smaller context
super().__init__(model_id="Qwen2.5-0.5B-Instruct-CPU")

# Better reasoning, larger context
super().__init__(model_id="Qwen3-Coder-30B-A3B-Instruct-GGUF")

# Different vision model
self.init_vlm(model="Qwen3-VL-8B-Instruct-GGUF")

# Photorealistic images (slower)
self.init_sd(default_model="SDXL-Base-1.0")
```

Models are downloaded automatically when first used.

---

## What's Next?

ðŸŽ‰ **Congratulations!** You've built your first multi-modal agent.

<Card title="Part 2: Architecture Deep Dive â†’" icon="diagram-project" href="/playbooks/sd-agent/part-2-architecture">
  Understand how multi-modal agents work. Learn about the reasoning loop, tool composition patterns, and mixin system.
</Card>
