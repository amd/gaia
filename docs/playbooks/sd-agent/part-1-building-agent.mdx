---
title: "Part 1: Build Your First Multi-Modal Agent"
description: "Build and run an agent that generates images and creates stories in 25 minutes"
icon: "rocket"
---

<Info>
  **Time to complete:** 25 minutes
  **Part 1 of 3** - [Overview](/playbooks/sd-agent) ‚Ä¢ Part 2 ‚Üí ‚Ä¢ Part 3 ‚Üí
</Info>

## What You'll Build

By the end of this part, you'll have a fully functional `ImageStoryAgent` class that demonstrates multi-modal AI composition.

**Technically, you're building:**

An agent class that inherits from three parent classes using Python's multiple inheritance:
```python
class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    pass  # ~62 lines of code total including CLI loop
```

**Inheritance chain (MRO):**
```
ImageStoryAgent ‚Üí Agent ‚Üí SDToolsMixin ‚Üí VLMToolsMixin ‚Üí object
```

When you call `super().__init__()`, Python walks this chain to initialize each parent cooperatively.

**What "multi-modal" means here:**

Your agent orchestrates three different AI models through a unified interface:

| Model | Role | Endpoint | Hardware | Context |
|-------|------|----------|----------|---------|
| **Qwen3-8B-GGUF** | Reasoning engine | `POST /v1/chat/completions` | iGPU (Radeon) | 8K tokens |
| **SDXL-Turbo** | Image generation | `POST /images/generations` | CPU | N/A |
| **Qwen3-VL-4B-Instruct-GGUF** | Vision analysis | `POST /v1/chat/completions` (with images) | iGPU (Radeon) | 8K tokens |

**How it works:**
1. Your agent's `process_query()` method sends user input to the **LLM**
2. LLM returns a plan: `[{tool: "generate_image", tool_args: {...}}, {tool: "create_story_from_image", tool_args: {...}}]`
3. Agent executes each tool in sequence:
   - `generate_image()` ‚Üí HTTP POST to Lemonade `/images/generations` ‚Üí Returns image bytes ‚Üí Saved to disk
   - `create_story_from_image()` ‚Üí HTTP POST to Lemonade `/v1/chat/completions` with image ‚Üí Returns story text
4. LLM synthesizes final answer combining all results

**Model formats and acceleration:**
- GGUF models (Qwen3-8B, Qwen3-VL-4B) ‚Üí Run on **iGPU (Radeon)** via Vulkan
- Safetensors model (SDXL-Turbo) ‚Üí Currently runs on **CPU** (GPU acceleration planned)

Total memory: ~15GB loaded.

**What you'll be able to do:**

```python
agent = ImageStoryAgent()
result = agent.process_query("create a cyberpunk street scene with a story")

# Agent automatically:
# 1. Calls generate_image() ‚Üí creates image in ~17s
# 2. Calls create_story_from_image() ‚Üí generates 2-3 paragraph narrative in ~15s
# 3. Returns both image path and story text in final answer
```

**The 6 tools your agent will have:**

From `SDToolsMixin` (registered in `init_sd()`):

- **`generate_image(prompt: str, model: Optional[str], size: Optional[str], steps: Optional[int], cfg_scale: Optional[float], seed: Optional[int]) -> Dict[str, Any]`**
  - Calls: `LemonadeClient.generate_image()` ‚Üí `POST /images/generations`
  - Returns: `{"status": "success", "image_path": "...", "model": "SDXL-Turbo", "generation_time_s": 17.2, "seed": 123456}`
  - Saves: PNG file to `output_dir`, adds to `self.sd_generations` list

- **`list_sd_models() -> Dict[str, Any]`**
  - Returns: Available models with speed/quality characteristics

- **`get_generation_history(limit: int = 10) -> Dict[str, Any]`**
  - Returns: Recent generations from `self.sd_generations` session state

From `VLMToolsMixin` (registered in `init_vlm()`):

- **`analyze_image(image_path: str, focus: str = "all") -> Dict[str, Any]`**
  - Calls: `VLMClient.analyze()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "description": "...", "focus_analysis": "..."}`

- **`create_story_from_image(image_path: str, story_style: str = "any") -> Dict[str, Any]`**
  - Calls: `VLMClient.create_story()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "story": "2-3 paragraph narrative", "description": "..."}`

- **`answer_question_about_image(image_path: str, question: str) -> Dict[str, Any]`**
  - Calls: `VLMClient.answer_question()` ‚Üí `POST /v1/chat/completions` with image
  - Returns: `{"status": "success", "answer": "...", "confidence": "high"}`

**How tools get registered (under the hood):**

When you call `self.init_sd()` and `self.init_vlm()`:

1. **`init_sd()` creates closure functions** with `@tool` decorator:
   ```python
   @tool(atomic=True, name="generate_image", description="...", parameters={...})
   def generate_image(prompt, model=None, ...):
       return self._generate_image(prompt, model, ...)  # Calls instance method
   ```

2. **The `@tool` decorator** (defined in `gaia.agents.base.tools`):
   - Wraps the function with metadata
   - Adds it to global `_TOOL_REGISTRY` dict: `{tool_name: {function, schema, ...}}`
   - Returns the wrapped function

3. **At runtime**, when LLM calls a tool:
   - Agent looks up tool in `_TOOL_REGISTRY` by name
   - Calls the function with LLM-provided arguments
   - Returns result to LLM for next reasoning step

**Instance state created:**

After initialization, your agent instance has:
```python
self.sd_client          # LemonadeClient(base_url="http://localhost:8000")
self.sd_output_dir      # Path("./generated_images")
self.sd_default_model   # "SDXL-Turbo"
self.sd_generations     # [] - List of dicts tracking generated images
self.vlm_client         # VLMClient(vlm_model="Qwen3-VL-4B-Instruct-GGUF")
self.vlm_model          # "Qwen3-VL-4B-Instruct-GGUF"
```

Plus all the `Agent` base class state (tool registry, conversation history, LLM client).

---

## Prerequisites

<Steps>
  <Step title="Complete setup">
    If you haven't already, follow the [Setup guide](/setup) to install prerequisites (uv, Lemonade Server, etc.).
  </Step>

  <Step title="Create project folder">
    ```bash
    mkdir my-sd-agent
    cd my-sd-agent
    ```
  </Step>

  <Step title="Create virtual environment">
    ```bash
    uv venv .venv --python 3.12
    ```

    <Note>uv will automatically download Python 3.12 if not already installed.</Note>

    Activate the environment:

    <Tabs>
      <Tab title="Linux / macOS">
        ```bash
        source .venv/bin/activate
        ```
      </Tab>
      <Tab title="Windows">
        ```bash
        .venv\Scripts\activate
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step title="Install GAIA">
    ```bash
    uv pip install amd-gaia
    ```
  </Step>

  <Step title="Initialize SD profile">
    ```bash
    gaia init --profile sd
    ```

    This command:
    1. Installs Lemonade Server (if not already installed)
    2. **Starts Lemonade Server automatically**
    3. Downloads three models (~15GB):
       - **SDXL-Turbo** (6.5GB) - Diffusion model in safetensors format
       - **Qwen3-8B-GGUF** (5GB) - Agent LLM in quantized GGUF format
       - **Qwen3-VL-4B-Instruct-GGUF** (3.2GB) - Vision LLM in quantized GGUF format
    4. Verifies the setup is working

    **Hardware acceleration:**
    - GGUF models run on iGPU (Radeon integrated graphics) via Vulkan
    - SDXL-Turbo currently runs on CPU (GPU acceleration planned)

    <video
      controls
      className="w-full rounded-lg"
      src="https://assets.amd-gaia.ai/videos/gaia-init-sd-profile.webm"
    />
    <Note>Video: Running `gaia init --profile sd` and watching models download</Note>
  </Step>
</Steps>

---

## Building the Agent

### Step 1: Class Definition

Create a new file `image_story_agent.py` and start with the class definition:

```python
from gaia.agents.base import Agent
from gaia.sd import SDToolsMixin
from gaia.vlm import VLMToolsMixin

class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    """
    Multi-modal agent that generates images and creates stories.

    Inherits from:
    - Agent: Base class providing reasoning loop, tool registry, LLM interface
    - SDToolsMixin: Adds Stable Diffusion image generation capabilities
    - VLMToolsMixin: Adds vision-language model analysis capabilities
    """
    pass
```

---

### Step 2: Implement `__init__` and System Prompt

Add the `__init__` method to initialize both mixins, plus the required abstract methods:

<Accordion title="Model Selection Reference">
Available SD models (set in `default_model` parameter):

| Model | Speed | Quality | Best For |
|-------|-------|---------|----------|
| **SDXL-Turbo** (default) | ~17s | ‚≠ê‚≠ê‚≠ê | Balanced speed/quality |
| **SD-Turbo** | ~13s | ‚≠ê‚≠ê | Rapid prototyping |
| **SDXL-Base-1.0** | ~9min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Photorealistic images |

Performance measured on AMD Ryzen AI MAX+ 395 with Radeon 8060S
</Accordion>

Now add three methods to the class:

```python
    def __init__(self, output_dir="./generated_images"):
        # Initialize Agent base class with Qwen3-8B model
        # (SD profile downloads this model, not the default Qwen3-Coder-30B)
        super().__init__(
            model_id="Qwen3-8B-GGUF",
            min_context_size=8192,  # 8K context sufficient for image workflows
        )

        # Initialize SD tools
        # Under the hood: Creates LemonadeClient, sets output directory,
        # and registers tools via @tool decorator (generate_image, list_sd_models, etc.)
        self.init_sd(
            output_dir=output_dir,
            default_model="SDXL-Turbo",  # Fast stylized model (4 steps, ~17s)
        )

        # Initialize VLM tools
        # Under the hood: Creates VLMClient for vision analysis,
        # and registers tools (analyze_image, create_story_from_image, etc.)
        self.init_vlm(model="Qwen3-VL-4B-Instruct-GGUF")

    def _register_tools(self):
        # Required abstract method from Agent base class
        # Mixins already registered their tools in init_sd() and init_vlm()
        # Only implement this if you need custom tools beyond what mixins provide
        pass

    def _get_system_prompt(self) -> str:
        # Use the real SDAgent prompts with research-backed prompt engineering
        # Tool schemas (parameters, models, descriptions) are automatically injected by Agent
        from gaia.agents.sd.prompts import (
            BASE_GUIDELINES,
            MODEL_SPECIFIC_PROMPTS,
            WORKFLOW_INSTRUCTIONS,
        )

        # Get model-specific enhancement guidelines for the default model
        model_specific = MODEL_SPECIFIC_PROMPTS.get(
            self.sd_default_model, MODEL_SPECIFIC_PROMPTS["SDXL-Turbo"]
        )

        return BASE_GUIDELINES + model_specific + WORKFLOW_INSTRUCTIONS
```

---

### Step 3: Add CLI Loop and Run

Complete the agent by adding a CLI loop. Here's the full `image_story_agent.py` file:

```python
from gaia.agents.base import Agent
from gaia.sd import SDToolsMixin
from gaia.vlm import VLMToolsMixin

class ImageStoryAgent(Agent, SDToolsMixin, VLMToolsMixin):
    """Multi-modal agent that generates images and creates stories."""

    def __init__(self, output_dir="./generated_images"):
        # Initialize Agent base class with Qwen3-8B model
        # (SD profile downloads this model, not the default Qwen3-Coder-30B)
        super().__init__(
            model_id="Qwen3-8B-GGUF",
            min_context_size=8192,  # 8K context sufficient for image workflows
        )

        # Step 2: Initialize SD tools
        # Under the hood: Creates LemonadeClient, sets output directory,
        # and registers tools via @tool decorator (generate_image, list_sd_models, etc.)
        self.init_sd(
            output_dir=output_dir,
            default_model="SDXL-Turbo",  # Fast stylized model (4 steps, ~17s)
        )

        # Step 3: Initialize VLM tools
        # Under the hood: Creates VLMClient for vision analysis,
        # and registers tools (analyze_image, create_story_from_image, etc.)
        self.init_vlm(model="Qwen3-VL-4B-Instruct-GGUF")

    def _register_tools(self):
        # Required abstract method from Agent base class
        # Mixins already registered their tools in init_sd() and init_vlm()
        # Only implement this if you need custom tools beyond what mixins provide
        pass

    def _get_system_prompt(self) -> str:
        # Use the real SDAgent prompts with research-backed prompt engineering
        # Tool schemas (parameters, models, descriptions) are automatically injected by Agent
        from gaia.agents.sd.prompts import (
            BASE_GUIDELINES,
            MODEL_SPECIFIC_PROMPTS,
            WORKFLOW_INSTRUCTIONS,
        )

        # Get model-specific enhancement guidelines for the default model
        model_specific = MODEL_SPECIFIC_PROMPTS.get(
            self.sd_default_model, MODEL_SPECIFIC_PROMPTS["SDXL-Turbo"]
        )

        return BASE_GUIDELINES + model_specific + WORKFLOW_INSTRUCTIONS


if __name__ == "__main__":
    import os

    # Create output directory for generated images
    os.makedirs("./generated_images", exist_ok=True)

    # Initialize the agent (this triggers all the __init__ steps above)
    agent = ImageStoryAgent()

    print("Image Story Agent ready! Type 'quit' to exit.\n")

    # Interactive CLI loop
    while True:
        user_input = input("You: ").strip()

        if user_input.lower() in ("quit", "exit", "q"):
            print("Goodbye!")
            break

        if not user_input:
            continue

        # Process the query through the agent's reasoning loop
        # Under the hood: LLM plans which tools to call, executes them, synthesizes response
        result = agent.process_query(user_input)

        # Display the agent's final answer
        if result.get("final_answer"):
            print(f"\nAgent: {result['final_answer']}\n")
        else:
            print("\nAgent: (no response)\n")
```

Run it:
```bash
python image_story_agent.py
```

Try these prompts:
```
You: generate an image of a robot exploring ancient ruins and tell me the story of what it discovers
You: create a sunset over mountains and write a short story about the scene
You: make a cyberpunk street scene and describe what's happening there
```

---

## Example Output

Here's what you'll see when you run the agent:

```
You: create a robot exploring ancient ruins

Agent: I'll generate that image for you.

[Generating image with SDXL-Turbo (4 steps, 512x512)...]
‚úì Image saved to ./generated_images/robot_exploring_ancient_ruins_SDXL-Turbo_20250202_143022.png

[Analyzing image and creating story...]

Here's a story about your image:

In the heart of a forgotten civilization, a small reconnaissance robot named
Unit-7 carefully navigates through towering stone pillars covered in mysterious
glyphs. Its optical sensors scan the intricate carvings, attempting to decode
the ancient language that has been silent for millennia.

The robot's metallic frame gleams in the filtered sunlight that breaks through
the overgrown canopy above. Each step is calculated, avoiding the treacherous
gaps between fallen columns and the thick vines that have claimed this place
as their own.

As Unit-7's processors work to piece together the historical puzzle, it transmits
its findings back to the research station, unaware that it may be the first
sentient being to witness these ruins in over a thousand years.
```

**Performance (AMD Ryzen AI MAX+ 395):**
- Image generation: ~17s (SDXL-Turbo, 512x512)
- VLM story creation: ~15-20s (Qwen3-VL-4B)
- **Total time: ~35-40s** for image + story

<video
  controls
  className="w-full rounded-lg"
  src="https://assets.amd-gaia.ai/videos/image-story-agent-demo.webm"
/>
<Note>Video: Running the ImageStoryAgent and generating image + story</Note>

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Connection Error: Cannot connect to Lemonade Server">
    **Error:** `LemonadeClientError: Cannot connect to http://localhost:8000`

    **Solution:** Lemonade Server isn't running. Check status:
    ```bash
    lemonade-server status
    ```

    If not running, `gaia init --profile sd` should have started it. Try restarting:
    ```bash
    lemonade-server serve
    ```
  </Accordion>

  <Accordion title="Model not found error">
    **Error:** `Model 'SDXL-Turbo' not found`

    **Solution:** Download missing models:
    ```bash
    lemonade-server pull SDXL-Turbo
    lemonade-server pull Qwen3-VL-4B-Instruct-GGUF
    ```

    Or re-run init:
    ```bash
    gaia init --profile sd
    ```
  </Accordion>

  <Accordion title="Import error: No module named 'gaia.sd'">
    **Error:** `ModuleNotFoundError: No module named 'gaia.sd'`

    **Solution:** Upgrade GAIA:
    ```bash
    uv pip install --upgrade amd-gaia
    ```

    Verify installation:
    ```bash
    python -c "from gaia.sd import SDToolsMixin; print('OK')"
    ```
  </Accordion>

  <Accordion title="How to get reproducible results">
    **Question:** How do I generate the same image twice?

    **Solution:** Use a fixed seed:

    CLI:
    ```bash
    gaia sd "robot kitten" --seed 42
    ```

    Python:
    ```python
    result = agent.process_query("generate robot kitten with seed 42")
    ```

    By default, images use random seeds for variety. Specify a seed for reproducibility.
  </Accordion>
</AccordionGroup>

---

## What's Next?

üéâ **Congratulations!** You've built your first multi-modal agent. It works, but *how* does it work?

<Card title="Part 2: Architecture Deep Dive ‚Üí" icon="diagram-project" href="/playbooks/sd-agent/part-2-architecture">
  Now that you've built it, understand how multi-modal agents work under the hood. Learn about the reasoning loop, tool composition patterns, and mixin system.
</Card>
